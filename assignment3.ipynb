{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*p.s.: I apologize for the weird PDF. Due to a problem in my VSCode, I first need to export the ipynb to HTML, and then convert to PDF.*"
      ],
      "metadata": {
        "id": "f_8f1AK9dFFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective"
      ],
      "metadata": {
        "id": "3IhY3YTUQCMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing is a hobby I've had for more than ten years now. I've written a lot during this time, from large novels to tiny tales, covering tutorials, essays, and formal reports.\n",
        "\n",
        "In the first assignment, I analyzed whether we could use Naive Bayes to distinguish pieces I wrote for different purposes. In the second, I tried to create an LSTM that wrote like me. Now, I compare the performances of multiple LSTMs, seeking to understand how different structures perform and whether there is a cap to how good their outputs can be. Ultimately, the question I seek to answer is: how similar to my writing can the text produced by an LSTM be?"
      ],
      "metadata": {
        "id": "EcW5bkhCQNzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Description"
      ],
      "metadata": {
        "id": "CDZVpnm1RH2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used is comprised of writing samples from my Tumblr and Medium blogs, which were be downloaded as HMTL files in both websites, and notes I had on iCloud. In total, 52 HTML files were gathered, each one representing a a single piece of writing. The files were manually labeled into six categories:\n",
        "\n",
        "1. Social criticism/opinion piece\n",
        "2. Poem\n",
        "3. Tale\n",
        "4. Tutorial\n",
        "5. Notes\n",
        "6. Novel\n",
        "\n",
        "Last, writing samples have different lengths, are from different times, and are all in Portuguese, which is my native language."
      ],
      "metadata": {
        "id": "_gIPxXfwRKzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset to Python"
      ],
      "metadata": {
        "id": "3jyP9Vs6Rep1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous assignment, I built the structure to parse the HTMLs and convert the text into a Pandas dataframe. I had exported this data as a CSV, which I am now re-uploading below."
      ],
      "metadata": {
        "id": "f3FxSkH1Rlo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads all libraries that will be used in this assignment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXJbL6-7NmHZ",
        "outputId": "17bb3f90-faa9-4a49-8987-7c840ecdce71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqb1KmlbGObG",
        "outputId": "5ce53b61-b245-4eea-aa3a-ea0e5d4539a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Category                                            Content\n",
            "0        poem                  Se o fim entrasse em minha vista.\n",
            "1        poem        Entre risos perguntando “do que se trata?”.\n",
            "2        poem                           Puto, velho, vigarista!.\n",
            "3        poem                  Por que não logo que tu me mata?.\n",
            "4        poem            Queima-me a pele e escalda-me a cabeça.\n",
            "...       ...                                                ...\n",
            "4643     poem                       Porém, fácil mesmo é morrer.\n",
            "4644     poem        Assim como uma semente plantada no inverno.\n",
            "4645     poem             Assim como um anjo nascido no Inferno.\n",
            "4646     poem       Assim como o amor que não se consegue viver.\n",
            "4647     poem  Talvez morrerei sem ter a chance, da verdade, ...\n",
            "\n",
            "[4648 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/10years.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "9m66_ERGS6zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation\n",
        "\n",
        "There are a few pre-processing steps that we took in the previous assignment and will take here again. Namely:\n",
        "\n",
        "1. Remove \"\\n\" marks, which are present in some of the poems\n",
        "2. Convert everything to lowercase\n",
        "3. Remove punctuation marks\n",
        "4. Remove signatures (\" — Felipe, March 1985\")\n",
        "\n",
        "While steps 1 and 4 are meant to enhance the qiality of the data, steps 2 and 3 try to make it more simple, reducing the space of options the model has to learn."
      ],
      "metadata": {
        "id": "ZbVkduVuS8ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Create an empty list to store the updated data\n",
        "updated_data = []\n",
        "\n",
        "# Iterate through each row in the DataFrame\n",
        "for idx, row in df.iterrows():\n",
        "    content = row['Content']\n",
        "\n",
        "    # Split the content by '\\n'\n",
        "    lines = content.split('\\n')\n",
        "\n",
        "    # Add each line as a separate entry\n",
        "    for line in lines:\n",
        "        if line.strip():  # Check if line is not empty (to avoid adding empty entries)\n",
        "            updated_data.append({'Category': row['Category'], 'Content': line.strip()})\n",
        "\n",
        "# Create a new DataFrame with the updated data\n",
        "updated_df = pd.DataFrame(updated_data)\n",
        "\n",
        "# Convert all entries to lowercase\n",
        "updated_df['Content'] = updated_df['Content'].str.lower()\n",
        "\n",
        "# Remove punctuation signs from the entries\n",
        "def remove_punctuation(text):\n",
        "    punctuation_to_remove = string.punctuation.replace('-', '').replace(' ', '')  # Keeps hyphens because in Portuguese they matter\n",
        "    return text.translate(str.maketrans('', '', punctuation_to_remove))\n",
        "\n",
        "updated_df['Content'] = updated_df['Content'].apply(remove_punctuation)\n",
        "\n",
        "# Remove entries containing signatures or\n",
        "updated_df = updated_df[~updated_df['Content'].str.contains('bandeira')]\n",
        "updated_df = updated_df[~updated_df['Content'].str.contains('poema')]\n",
        "updated_df = updated_df[~updated_df['Content'].str.contains('tumblr')]\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(updated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCgM6IHbLk-i",
        "outputId": "4a1fbad3-d935-4a8c-c836-28df6f285ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Category                                            Content\n",
            "0        poem                   se o fim entrasse em minha vista\n",
            "1        poem          entre risos perguntando “do que se trata”\n",
            "2        poem                               puto velho vigarista\n",
            "3        poem                    por que não logo que tu me mata\n",
            "4        poem             queima-me a pele e escalda-me a cabeça\n",
            "...       ...                                                ...\n",
            "4933     poem                         porém fácil mesmo é morrer\n",
            "4934     poem         assim como uma semente plantada no inverno\n",
            "4935     poem              assim como um anjo nascido no inferno\n",
            "4936     poem        assim como o amor que não se consegue viver\n",
            "4937     poem  talvez morrerei sem ter a chance da verdade co...\n",
            "\n",
            "[4856 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory analysis"
      ],
      "metadata": {
        "id": "tu4hsaKkVMNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can start by checking the size of the dataset and whether it is imbalanced. Below, we can see that there are 2.5x more novel entries than poems and tales, which in turns are 3x more present than entries from notes and tutorials. With such an imbalanced dataset, we outght to be mindful of implications this might have in whatever model we build."
      ],
      "metadata": {
        "id": "OuR0pM7mVcFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = updated_df['Category'].value_counts()\n",
        "print(label_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXJad9p2VPRz",
        "outputId": "d27863b6-ea58-44dc-b1d1-c42cce72f243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "novel       2281\n",
            "poem         953\n",
            "tale         915\n",
            "notes        335\n",
            "tutorial     308\n",
            "opinion       64\n",
            "Name: Category, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look into how big the dataset is from a word-based perspective, given that entries can vary in size. The results are below, but to illustrate it clearly: if we were to condense the entire dataset in a single document, it would fill 83 pages in Arial 11 font. Not bad for a hobby!"
      ],
      "metadata": {
        "id": "H462V6ZyW9r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(dataframe, column_name):\n",
        "    total_words = 0\n",
        "    unique_words = set()\n",
        "    for text in dataframe[column_name]:\n",
        "        words = text.split()\n",
        "        total_words += len(words)\n",
        "        unique_words.update(set(words))\n",
        "    return total_words, len(unique_words)\n",
        "\n",
        "# Usage:\n",
        "total_words, unique = count_words(updated_df, 'Content')\n",
        "\n",
        "print(f\"Total number of words in the dataset: {total_words}\")\n",
        "print(f\"Number of unique words: {unique}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lecYRWomWIjY",
        "outputId": "d1602ed2-1d3d-490e-aff3-9fe352e89639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in the dataset: 57997\n",
            "Number of unique words: 8860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last, by printing a few random samples of data, we can better observe how each entry looks:"
      ],
      "metadata": {
        "id": "o-hLuQY3T_Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select observations\n",
        "random_observations = updated_df.sample(n=10)\n",
        "\n",
        "# Print the content of the selected observations\n",
        "for idx, row in random_observations.iterrows():\n",
        "    print(f\"Content: {row['Content']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlTm10U3Tv-n",
        "outputId": "7cb12989-0c93-41d9-94db-be8ceef5cc32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content: poucas sensações se igualavam àquilo\n",
            "Content: ao entrar sentiu o cheiro da carcaça de carne que apodrecia ali dentro\n",
            "Content: parasita maníaco e solitário\n",
            "Content: seus olhos ficam molhados a garganta dói mas você não chora\n",
            "Content: e não existe nem uma remota chance de você sair andando daqui se me der de muito longe como resposta- venho diretamente do inferno seth- e me diga por que é que você está fugindo da sua esposa\n",
            "Content: já conseguisse passar algum dia da tua vida sem ter que apelar pra essa noia tua de esperança\n",
            "Content: eu posso sentir\n",
            "Content: faça-me um carinho e me dê um beijo depois\n",
            "Content: via-se dentro do próprio caixão\n",
            "Content: as origens confidenciadas a fat jack não muito tempo antes agora eram usadas contra si próprio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task Explanation and Data Split"
      ],
      "metadata": {
        "id": "c9VzdWMpXe5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task explanation"
      ],
      "metadata": {
        "id": "C_uVP15dv-26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After years of writing, I want to have the experience of being a reader of myself. Of course, reading something I actually wrote would not be enough because I know what's coming next, which makes it impossible for me to feel surprise or enjoy the novelty of a text. This is why I want to train a model that can *write something new* like me.\n",
        "\n",
        "Technically speaking, this means training a model to understand the subtle patterns in my writing style to an extent that it is capable of generalizing them. However, there are multiple ways of doing so, and in this process a natural question arises: within a base structure (here, LSTMs), is there an architecture that performs best?"
      ],
      "metadata": {
        "id": "jzslqvK1w3g6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model selection"
      ],
      "metadata": {
        "id": "hj5YWhgI0Ru4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To accomplish the task of text generation, I chose to built a Long Short-Term Memory (LSTM) network. LSTMs are a type of RNN (Recurrent Neural Network) designed to capture long-term dependencies in sequential data.\n",
        "\n",
        "The way LSTMs work can be illustrated with the analogy of reading a book and trying to understand the plot: as we read the pages, we continuously update our understanding based on the current sentence and what we've read previously. LSTM does a similar process, but using numerical data instead of words. As in any neural network, each layer takes in some input, applies a set of weights, and produces an output. However, in an RNN (and consequently in an LSTM), there's a hidden state that's passed along from one step to the next. This hidden state acts like a memory, allowing the network to consider past information while processing current input. The difference between RNNs and LSTMs is that the latter is better at handling memory, suffering less from vanishing gradients when the input becomes large."
      ],
      "metadata": {
        "id": "9eR_ZxWP5uXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each time a new input is given to the model, the following process happens:\n",
        "<br><br>\n",
        "\n",
        "#### **1) Deciding how much of the long-term memory to forget**\n",
        "The first part of LSTM (named Forget Gate) determines how much of the long term memory should be remembered for the current calculation. To do so, it uses the short-term memory ($h_{t-1}$) and the current input ($x_t$), returning a percentage $f_t$ that will be factored in the long-term memory later.\n",
        "\n",
        "$$\n",
        "f_t = \\sigma(W_{f} \\cdot [h_{t-1}, x_t] + b_f)\n",
        "$$\n",
        "\n",
        "in which any $W$ is a weight matrix and any $b$ is a bias vector.\n",
        "\n",
        "#### **2) Deciding what to add to the long-term memory**\n",
        "Next, the LSTM combines the short-term memory with the input to create a potential long-term memory, $C^\\sim_t$.\n",
        "\n",
        "$$\n",
        "C^\\sim_t = \\text{tanh}(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
        "$$\n",
        "\n",
        "\n",
        "Then, it determines what percentage of this potential memory should be actually incorporated into the long-term memory. This entire process happens on what is called the Input Gate.\n",
        "\n",
        "$$\n",
        "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
        "$$\n",
        "\n",
        "Following these steps, we update the long-term memory, $C_t$, based on the previous memory (and the amount of it we dediced to forget), and the candidate new memory (along with the amount we decided to remember):\n",
        "\n",
        "$$\n",
        "C_t = f_t \\star C_{t-1} + i_t \\star C^\\sim_t\n",
        "$$\n",
        "\n",
        "#### **3) Deciding what to output**\n",
        "\n",
        "Last, we output a value by first combining the short-term memory and the input, which gives us a candidate output $o_t$:\n",
        "\n",
        "$$\n",
        "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
        "$$\n",
        "\n",
        "And then we factor in our long-term memory, thus obtaining the final output. Given that this output will be the short-term memory for the next input, we call it $h_t$:\n",
        "\n",
        "$$\n",
        "h_t = o_t \\star \\text{tanh}(C_t)\n",
        "$$\n",
        "\n",
        "\n",
        "<br>\n",
        "<i>(Note: In a LSTM, everything we have just described is a single neuron)"
      ],
      "metadata": {
        "id": "QgFcujF98A75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weights and biases are randomly initialized and updated through backpropagation."
      ],
      "metadata": {
        "id": "7p6RhaXM8l0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "fIG1RKPTwCDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a model that can write like me, I am assuming that the piece category (poem, tale...) doesn't matter, which means all of the data can be grouped together. We thus start by converting all of the text into a single string."
      ],
      "metadata": {
        "id": "jzxD2z-Wxsj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = updated_df['Content'].str.cat(sep=' ')"
      ],
      "metadata": {
        "id": "zYf6xR2_LMk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we map the characters of the vocabulary to integers. Given that LSTMs are made to work with numerical data, each character in the text needs to be represented as a numerical value. This mapping allows us to process characters through the model, and later to reverse the process and convert numerical outputs into text again."
      ],
      "metadata": {
        "id": "t9TO1FpJx2fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters:\", n_chars)\n",
        "print(\"Total Vocab:\", n_vocab)\n",
        "print(f\"Characters that compose the vocabulary: {chars}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSv-2VkRM5nJ",
        "outputId": "451d1850-a079-416d-8a4e-b19f0c1cd3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters: 322745\n",
            "Total Vocab: 58\n",
            "Characters that compose the vocabulary: [' ', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', 'à', 'á', 'â', 'ã', 'ç', 'é', 'ê', 'í', 'ó', 'ô', 'õ', 'ú', '\\u200a', '–', '—', '’', '“', '”', '…']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we split the data into inpt-output pairs. We want the model to predict one character at a time based on the previous 100 characters. Therefore, our input will be a sequence of 100 characters starting in $i$ and finishing in $i+99$, and the output, a sequence of 100 characters starting in $i+1$ and finishing in $i+100$."
      ],
      "metadata": {
        "id": "hqbPirLm3PYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        " seq_in = raw_text[i:i + seq_length]\n",
        " seq_out = raw_text[i + seq_length]\n",
        " dataX.append([char_to_int[char] for char in seq_in])\n",
        " dataY.append(char_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIBqxzYHNU2U",
        "outputId": "1b23172b-8a71-487c-db2f-7d21f63b82a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  322645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last, we reshape the input to the format expected by Keras, normalize it, and convert the output to 58-dimensional vectors (the size of the vocabulary). This means that, after processing the data, the LSTM will output a vector with probabilities for the next letter.  "
      ],
      "metadata": {
        "id": "87jhqPPc44cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "cdCX17NHNexw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Initialization"
      ],
      "metadata": {
        "id": "K5D34hRDN-cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we initialize our base LSTM model. We have two layers with 256 neurons each, two dropout layers in between to prevent overfitting, and a softmax at the end. Additionally, the fact we are using stacked LSTMs should also increase our capacity to represent more complex inputs."
      ],
      "metadata": {
        "id": "oyHhOWMc8y6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "kvB2WttwKZCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we initialize an LSTM with the same structure, but more neurons. Theoretically, the greater number of neurons should allow the model to capture more information and patterns."
      ],
      "metadata": {
        "id": "XBQnQoTxNlIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates LSTM model\n",
        "larger_model = Sequential()\n",
        "larger_model.add(LSTM(768, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "larger_model.add(Dropout(0.25))\n",
        "larger_model.add(LSTM(768))\n",
        "larger_model.add(Dropout(0.25))\n",
        "larger_model.add(Dense(y.shape[1], activation='softmax'))\n",
        "larger_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "9efa07wWNdpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, we adjust the base model to predict words, instead of characters. Although this increases considerably the number of different inputs the model can take (and patterns it needs to learn), it might make it easier for the model to connect words together in a coherent way.\n",
        "\n",
        "*(Given that this model takes a different input, we need to retokenize the data before initializing the model)*"
      ],
      "metadata": {
        "id": "SZMlj9-aN_eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate text data\n",
        "raw_text = updated_df['Content'].str.cat(sep=' ')\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([raw_text])\n",
        "sequences = tokenizer.texts_to_sequences([raw_text])[0]\n",
        "total_words = len(tokenizer.word_index) + 1  # Adding 1 for Out of Vocabulary (OOV) token\n",
        "\n",
        "# Prepare sequences of 30 words as input and one word as output\n",
        "seq_length = 30\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(seq_length, len(sequences)):\n",
        "    seq_in = sequences[i - seq_length:i]\n",
        "    seq_out = sequences[i]\n",
        "    dataX.append(seq_in)\n",
        "    dataY.append(seq_out)\n",
        "\n",
        "# Convert the sequences into numpy arrays\n",
        "X = np.array(dataX)\n",
        "y = to_categorical(dataY, num_classes=total_words)\n",
        "\n",
        "print(\"Total Sequences: \", len(dataX))\n",
        "\n",
        "# Now, X contains sequences of 30 words, and y is the one-hot encoded output.\n",
        "# These can be used for training the LSTM model.\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "n_patterns = len(dataX)\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(total_words)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "U9YutdFYNdmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusting the model for word-level prediction\n",
        "words_model = Sequential()\n",
        "words_model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "words_model.add(Dropout(0.2))\n",
        "words_model.add(LSTM(256))  # No return_sequences needed in the last LSTM layer\n",
        "words_model.add(Dropout(0.2))\n",
        "words_model.add(Dense(total_words, activation='softmax'))  # Changed y.shape[1] to total_words\n",
        "words_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "mfrEFlS-Ndkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ayy6s2P_OfUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the base model, we trained for 70 epochs and a batch size of 60 (which means 60 training samples will be passed through the network before we update weights with backpropagation). With the resources from Colab Free, the training took 1h56min, achieving a minimum loss of 1.4405."
      ],
      "metadata": {
        "id": "Jgo3xjUTvkfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/model/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=70, batch_size=60, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "LZNS__yDOLVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the model with extra neurons, we trained for 20 epochs before Colab free shut itself down, achieving a minimum loss of 1.2908."
      ],
      "metadata": {
        "id": "Cj0PTQEMOrw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# fit the model\n",
        "larger_model.fit(X, y, epochs=70, batch_size=60, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "4bgZJ4_cOzY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last, for the model predicting words, the training lasted 3 hours, covering 300 epochs with a batch size of 15, which yielded a 0.5208 loss."
      ],
      "metadata": {
        "id": "NwRRTV3EPBQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"/content/drive/MyDrive/156 materials/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "words_model.fit(X, y, epochs=300, batch_size=15, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "4jr8kq13PRbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Predictions"
      ],
      "metadata": {
        "id": "qKWtz112BsaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the models trained and their best version saved, we can now upload the weights and ask each one to generate new text based on an initial sample from the dataset. Below we have the functions for such."
      ],
      "metadata": {
        "id": "LHeqWi-SBu2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "filename = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/model/weights-improvement-70-1.4405-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "filename_larger = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-20-1.2908-bigger.hdf5\"\n",
        "larger_model.load_weights(filename_larger)\n",
        "larger_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "JhCkyy8AWHCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar to how we initialy converted letters to numbers, we now do the opposite proces\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "jbhzK_aAPQVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_generate(model):\n",
        "  # pick a random seed\n",
        "  start = np.random.randint(0, len(dataX)-1)\n",
        "  pattern = dataX[start]\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  print(\"\\nModel generation:\")\n",
        "\n",
        "  print_results(pattern, model)\n",
        "\n",
        "def custom_generate(text):\n",
        "  lst = []\n",
        "  lst.append([char_to_int[char] for char in text])\n",
        "  pattern = lst[0]\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  print(\"\\nModel generation:\")\n",
        "\n",
        "  print_results(pattern, model)\n",
        "\n",
        "\n",
        "def print_results(pattern, model):\n",
        "  # generate characters\n",
        "  for i in range(400):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "  print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "K2WDYprZPXFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantitative performance metrics\n",
        "\n",
        "Unlike classification or regression tasks, there apparently are few metrics to evaluate the performance of generative models. Some of the most common ones are:\n",
        "\n",
        "1. **BLEU Score**: commonly used in translation tasks, it computes the similarity between the generated text and a set of reference (human-generated) texts.<br>\n",
        "2. **Perplexity**: measures how well a model predicts a sample of text.\n",
        "3. **ROGUE**: commonly used in text summarization, it evaluates the quality of summaries or generated text by measuring the overlap in n-grams (sequences of words) between the generated and the reference texts.\n",
        "\n",
        "None of these metrics seem to be applicable in our case, as our goal is to mimic the writing style of the original dataset while producing novel work (which is very difficult to evaluate)."
      ],
      "metadata": {
        "id": "FCN3KlYPWmWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qualitative performance metrics\n",
        "Given the lack of a quantitative metric, we will evaluate the model qualitatively, using the best possible evaluation method: the author's opinion of the model's output. The sampled seed and output from the next code cell are translated below. <br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "pLUZ6nI0CWg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Base model"
      ],
      "metadata": {
        "id": "qvUodjLNPgR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auto_generate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqGEP3aKGKSv",
        "outputId": "bc12e6d8-2a0f-408c-9488-8dc012e5274b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" brilhar teu raciocínio rápido esperto sempre fizera eu me esforçar para poder de alguma forma fazer  \"\n",
            "\n",
            "Model generation:\n",
            "a carne a lhe perfurar si próprio olhava buscando a lembrança como a cabeça do corpo de seu contexto de seu contexto que estava a conteguir a cada posta de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a c\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Sampled seed text:\n",
        "\n",
        "> * \"[..] shine your smart quick thinking has always made me strive to be able to somehow do\"\n",
        "\n",
        "> Generated output:\n",
        "\n",
        "> - \"your flesh pierced himself looked at seeking the memory as the head of the body of his context of his context that he was comtaining at each slice of his post the first time the mind if for even more than the containment of his post the first time mind no matter how much the contention of its post the first time the mind no matter what the contention of its post the first time the mind no matter what the c\""
      ],
      "metadata": {
        "id": "118_J5GeZHnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we try customizing the input:"
      ],
      "metadata": {
        "id": "oJf88OlbZU1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_generate(\"ontem foi um lindo dia o mar brilhava em olinda enquanto o sol iluminava as colinas e \", model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRcbEew4J4ia",
        "outputId": "d93d0654-4c00-406e-da87-aa0665fbbe6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" ontem foi um lindo dia o mar brilhava em olinda enquanto o sol iluminava as colinas e  \"\n",
            "\n",
            "Model generation:\n",
            "o context que estavam por causa de seu posto não se poderia estar e o context que estava a conteguir a cada posta de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a \n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Custom seed:\n",
        "> - yesterday was a beautiful day, the sea shone in olinda while the sun illuminated the hills and\n",
        "\n",
        "> Output:\n",
        "> - the context they were in because of their position could not be and the context that they were containing at each position of their position the first time the mind no matter how much the contention of their position the first time the mind no matter what the containment of its post the first time the mind no matter how much more than the containment of its post the first time"
      ],
      "metadata": {
        "id": "m7lMuOroNMHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of results\n",
        "\n",
        "For the automatic generation, while the seed text comes from a love poem I once wrote, the output seems to be a mix of words from a horror tale and generative hallucinations. There is also a lot of repetitiveness from the middle to the end of the output. In the customized generation, the output seems to mix words from different pieces that do not make sense together. Additionally, we again can see a repetitive pattern in this output, suggesting a bias from the model that will be in most outputs (I won't include extra outputs here, but in fact, it is)."
      ],
      "metadata": {
        "id": "9I1QNi1iP7Dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Extra neurons model"
      ],
      "metadata": {
        "id": "mkjBQBw0PiqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auto_generate(larger_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESTkQl012VND",
        "outputId": "2f74fc1e-7ffa-4fde-e1a5-947a5ab9544e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" pode encontrar o código desse meu aplicativo aqui enfim terminamos espero muito que o artigo tenha s \"\n",
            "\n",
            "Model generation:\n",
            "ido de madeira ertava entrar no casal para o contrabandista não era mais puro srabalho de seu app e a menos de se alguma manhira pue estava em seu rosto de tm celes  fmi a única coisa que estava en cada novo trabalho na parte de corrado para o casal para o contrabandista não era mais puro srabalho d\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Sampled seed text:\n",
        "\n",
        "> * \"you can find the code for my application here finally we're done I really hope this article has s\"\n",
        "\n",
        "> Generated output:\n",
        "\n",
        "> - \"gone from wood to eter the couple for the smuggler was no longer pure swork of his app and unless some maner pue was in his face of tm celes fmi the only thing that was inn each new work in the part of corrado for the couple For the smuggler it was no longer pure work of art.\""
      ],
      "metadata": {
        "id": "bn4riEw_5qLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of results\n",
        "\n",
        "This model performed worst than the previous one. It is capable of outputting characters in sequences that resemble words (placing spaces correctly, alternating between vowels and consonants...), but it often makes grammatical mistakes. Furthermore, the words it generates correctly do not make sense — unlike the previous model, neighboring words have little connection with each other.\n",
        "\n",
        "Last, during training, the loss improved very little from epoch to epoch, suggesting that longer training times would not contribute much for improving the quality of the output."
      ],
      "metadata": {
        "id": "x8KqmeHU7SO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Words-based model"
      ],
      "metadata": {
        "id": "B2gexWCdQl70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final set of weights from epoch 300 was highly overfit, so instead I used for prediction a set of weights from epoch 90, which had captured some patterns from my writing but wasn't copying the training data yet."
      ],
      "metadata": {
        "id": "YatCvvMBYYpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads weights\n",
        "filename_words = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/words_model/weights-improvement-91-1.7459-bigger.hdf5\"\n",
        "words_model.load_weights(filename_words)\n",
        "words_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "huqRe1-uYYKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "int_to_word = {index: word for word, index in word_index.items()}\n",
        "\n",
        "def generate_text_words(words_model, sequences, tokenizer, seq_length, total_words, int_to_word, num_words=50):\n",
        "    # pick a random seed\n",
        "    start = np.random.randint(0, len(sequences)-1)\n",
        "    pattern = sequences[start-seq_length:start]\n",
        "    print(\"Seed:\")\n",
        "    print(\" \".join([int_to_word[value] for value in pattern]))\n",
        "    print(\"\\nModel generation:\")\n",
        "\n",
        "    # generate words\n",
        "    for i in range(num_words):\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(total_words)\n",
        "        prediction = words_model.predict(x, verbose=0)\n",
        "        index = np.argmax(prediction)\n",
        "        result = int_to_word[index]\n",
        "        sys.stdout.write(result + \" \")\n",
        "        pattern = np.append(pattern, index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "\n",
        "\n",
        "# Calling the function to generate text using the word-based model\n",
        "generate_text_words(words_model, sequences, tokenizer, seq_length, total_words, int_to_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZftCMO5hZHz",
        "outputId": "4dd8a287-f7db-42e2-b2be-7e27463c55f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "células com conteúdo para isso usaremos outro protocolo de collection view mas antes precisamos de uma breve explicação imagine que tenhamos 10000 itens para exibir na cv se continuássemos implementando\n",
            "\n",
            "Model generation:\n",
            "os código normalmente iríamos criar uma célula de cada mais de 10000 itens mesmo ter nem células de dor e olhos que jeito o noite de deixar e deixar estava uma não três não o não estridente não ruas do bar de o vai que rosto que o tempo futuro \n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Sampled seed text:\n",
        "\n",
        "> * \"cells with content for this we will use another collection view protocol but first we need a brief explanation imagine we have 10000 items to display in the cv if we continued implementing\"\n",
        "\n",
        "> Generated output:\n",
        "\n",
        "> - \"the codes normally we would create a cell of each more than 10000 items even though there are no pain cells and eyes that way the night of leaving and leaving was one no three no the no strident no streets of the bar of the go what face that the future time\""
      ],
      "metadata": {
        "id": "-uKAhGn98S-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of results\n",
        "\n",
        "There are no grammatical mistakes here — which is expected, since the model is trained on words, and not characters. However, the output is a mix of overfit text and hallucinations. The beginning of the output is the exact continuation of the input, which comes from an iOS tutorial I once wrote. However, at some point, it switches to a nearly-random set of words. This random set resembles some of the novels I wrote, but they don't make sense.\n",
        "\n",
        "It is worth mentioning again that this result comes from the weights the model had around epoch 90. Weights from earlier epochs resulted in text with no meaning, and from later epochs, in copies of the training data due to overfit.\n",
        "\n",
        "Overall, the model trained on words seems to be unable to find the balance between learning my writing style, learning to generate text that makes sense, and not overfitting the training data."
      ],
      "metadata": {
        "id": "6NCoasQH9bHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion of Results"
      ],
      "metadata": {
        "id": "srdpGZaea2Lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the character-predicting LSTMs are generating words correctly and hardly ever misspell any. This is, first of all, surprising, given that our models are simply predicting one character at a time. What we see is that it samples letters in a way that makes sense — for example, it doesn't sample a list of 20 consecutive letter, nor does it sample things like \"yzgsfat\" — and these samples turn out to have meaning to us, being words we actually understand. Additionally, they sometimes even present words that could make sense together, such as \"because of their position\" or \"the mind becomes more than\".\n",
        "\n",
        "However, the sentences the LSTMs build are not really logical, and we are left with an output that resembles real language, but is not. One might argue that the performance could have been enhanced if we had trained for longer, but most likely, it seems that there isn't much room for improvement.For the word-based model, results are a bit better, but far from optimal. Even when balancing underfitting and overfitting, the output does not carry much meaning and, after a few words, makes no sense to the reader.\n",
        "\n",
        "\n",
        "All in all, it seems that LSTMs can only scratch the surface of text generation. These models produce outputs that individually makes sense (such as characters that make up actual words), but they struggle to arrange these successful units in a meaningful way, apparently being unable to create useful sentences without overfitting the training data.\n",
        "\n",
        "In order to bridge this gap, we need a model that can understand the relevance of each word relative to each other. This likely means a model that contains attention mechanisms, such as Transformer-based architectures, which inherit all good features from LSTMs (such as memory and the capacity of processing sequential inputs) and more."
      ],
      "metadata": {
        "id": "DrxGUctrZX9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executive Summary\n",
        "\n",
        "In this assignment, I explored the extent to which LSTMs can mock my way of writing. I started by importing the data from my last 10 years of writing, cleaning it, and preparing it to be processed by LSTMs. Then, I explained why LSTMs were a good choice, describing their advantage over RNNs and providing a step-by-step explanation of how they work. Next, I initialized three different LSTMs and trained them on my dataset, keeping track of the loss and saving the weights of the best performing model. Finally, I used the trained model to make inferences on both samples from the dataset and customized inputs.\n",
        "\n",
        "Evaluating generative models like this seems to be a topic of debate on current research and, unlike with classification models, in which there are well-established metrics for performance evaluation, this does not seem to be the case here. Consequently, my analysis of their performance was based on a personal assessment of how closely the models' output resembled by own writing. While it was fascinating to see that the models could output words correctly, they were not capable of generating long chunks of text that made sense."
      ],
      "metadata": {
        "id": "WQ111xSPbbQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "CNvTs2gbbdT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial for LSTM for text generation:\n",
        "- https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "How LSTMs work:\n",
        "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "- https://www.youtube.com/watch?v=YCzL96nL7j0\n",
        "\n",
        "How to evaluate generative models:\n",
        "- https://saturncloud.io/glossary/evaluating-generative-models/\n",
        "- https://arxiv.org/abs/2206.10935\n",
        "- ChatGPT"
      ],
      "metadata": {
        "id": "S8FgYzHxcFA8"
      }
    }
  ]
}