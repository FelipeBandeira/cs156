{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_8f1AK9dFFh"
      },
      "source": [
        "*p.s.: I apologize for the weird PDF. Due to a problem in my VSCode, I first need to export the ipynb to HTML, and then convert to PDF.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IhY3YTUQCMZ"
      },
      "source": [
        "# Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcW5bkhCQNzg"
      },
      "source": [
        "Writing is a hobby I've had for more than ten years now. I've written a lot during this time, from large novels to tiny tales, covering tutorials, essays, and formal reports.\n",
        "\n",
        "In the first assignment, I analyzed whether we could use Naive Bayes to distinguish pieces I wrote for different purposes. Now, I take a step further to explore: can I train a simple model that mock my way of writing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDZVpnm1RH2y"
      },
      "source": [
        "# Dataset Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gIPxXfwRKzf"
      },
      "source": [
        "The dataset used is comprised of writing samples from my Tumblr and Medium blogs, which were be downloaded as HMTL files in both websites, and notes I had on iCloud. In total, 52 HTML files were gathered, each one representing a a single piece of writing. The files were manually labeled into six categories:\n",
        "\n",
        "1. Social criticism/opinion piece\n",
        "2. Poem\n",
        "3. Tale\n",
        "4. Tutorial\n",
        "5. Notes\n",
        "6. Novel\n",
        "\n",
        "Last, writing samples have different lengths, are from different times, and are all in Portuguese, which is my native language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jyP9Vs6Rep1"
      },
      "source": [
        "# Importing Dataset to Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3FxSkH1Rlo_"
      },
      "source": [
        "In the previous assignment, I built the structure to parse the HTMLs and convert the text into a Pandas dataframe. I had exported this data as a CSV, which I am now re-uploading below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXJbL6-7NmHZ",
        "outputId": "17bb3f90-faa9-4a49-8987-7c840ecdce71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Loads all libraries that will be used in this assignment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqb1KmlbGObG",
        "outputId": "5ce53b61-b245-4eea-aa3a-ea0e5d4539a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Category                                            Content\n",
            "0        poem                  Se o fim entrasse em minha vista.\n",
            "1        poem        Entre risos perguntando “do que se trata?”.\n",
            "2        poem                           Puto, velho, vigarista!.\n",
            "3        poem                  Por que não logo que tu me mata?.\n",
            "4        poem            Queima-me a pele e escalda-me a cabeça.\n",
            "...       ...                                                ...\n",
            "4643     poem                       Porém, fácil mesmo é morrer.\n",
            "4644     poem        Assim como uma semente plantada no inverno.\n",
            "4645     poem             Assim como um anjo nascido no Inferno.\n",
            "4646     poem       Assim como o amor que não se consegue viver.\n",
            "4647     poem  Talvez morrerei sem ter a chance, da verdade, ...\n",
            "\n",
            "[4648 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/10years.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m66_ERGS6zm"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbVkduVuS8ee"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "There are a few pre-processing steps that we took in the previous assignment and will take here again. Namely:\n",
        "\n",
        "1. Remove \"\\n\" marks, which are present in some of the poems\n",
        "2. Convert everything to lowercase\n",
        "3. Remove punctuation marks\n",
        "4. Remove signatures (\" — Felipe, March 1985\")\n",
        "\n",
        "While steps 1 and 4 are meant to enhance the qiality of the data, steps 2 and 3 try to make it more simple, reducing the space of options the model has to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCgM6IHbLk-i",
        "outputId": "4a1fbad3-d935-4a8c-c836-28df6f285ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Category                                            Content\n",
            "0        poem                   se o fim entrasse em minha vista\n",
            "1        poem          entre risos perguntando “do que se trata”\n",
            "2        poem                               puto velho vigarista\n",
            "3        poem                    por que não logo que tu me mata\n",
            "4        poem             queima-me a pele e escalda-me a cabeça\n",
            "...       ...                                                ...\n",
            "4933     poem                         porém fácil mesmo é morrer\n",
            "4934     poem         assim como uma semente plantada no inverno\n",
            "4935     poem              assim como um anjo nascido no inferno\n",
            "4936     poem        assim como o amor que não se consegue viver\n",
            "4937     poem  talvez morrerei sem ter a chance da verdade co...\n",
            "\n",
            "[4856 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "# Create an empty list to store the updated data\n",
        "updated_data = []\n",
        "\n",
        "# Iterate through each row in the DataFrame\n",
        "for idx, row in df.iterrows():\n",
        "    content = row['Content']\n",
        "\n",
        "    # Split the content by '\\n'\n",
        "    lines = content.split('\\n')\n",
        "\n",
        "    # Add each line as a separate entry\n",
        "    for line in lines:\n",
        "        if line.strip():  # Check if line is not empty (to avoid adding empty entries)\n",
        "            updated_data.append({'Category': row['Category'], 'Content': line.strip()})\n",
        "\n",
        "# Create a new DataFrame with the updated data\n",
        "updated_df = pd.DataFrame(updated_data)\n",
        "\n",
        "# Convert all entries to lowercase\n",
        "updated_df['Content'] = updated_df['Content'].str.lower()\n",
        "\n",
        "# Remove punctuation signs from the entries\n",
        "def remove_punctuation(text):\n",
        "    punctuation_to_remove = string.punctuation.replace('-', '').replace(' ', '')  # Keeps hyphens because in Portuguese they matter\n",
        "    return text.translate(str.maketrans('', '', punctuation_to_remove))\n",
        "\n",
        "updated_df['Content'] = updated_df['Content'].apply(remove_punctuation)\n",
        "\n",
        "# Remove entries containing signatures or\n",
        "updated_df = updated_df[~updated_df['Content'].str.contains('bandeira')]\n",
        "updated_df = updated_df[~updated_df['Content'].str.contains('poema')]\n",
        "updated_df = updated_df[~updated_df['Content'].str.contains('tumblr')]\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(updated_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu4hsaKkVMNm"
      },
      "source": [
        "## Exploratory analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuR0pM7mVcFk"
      },
      "source": [
        "We can start by checking the size of the dataset and whether it is imbalanced. Below, we can see that there are 2.5x more novel entries than poems and tales, which in turns are 3x more present than entries from notes and tutorials. With such an imbalanced dataset, we outght to be mindful of implications this might have in whatever model we build."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXJad9p2VPRz",
        "outputId": "d27863b6-ea58-44dc-b1d1-c42cce72f243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "novel       2281\n",
            "poem         953\n",
            "tale         915\n",
            "notes        335\n",
            "tutorial     308\n",
            "opinion       64\n",
            "Name: Category, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "label_counts = updated_df['Category'].value_counts()\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H462V6ZyW9r5"
      },
      "source": [
        "We can also look into how big the dataset is from a word-based perspective, given that entries can vary in size. The results are below, but to illustrate it clearly: if we were to condense the entire dataset in a single document, it would fill 83 pages in Arial 11 font. Not bad for a hobby!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lecYRWomWIjY",
        "outputId": "d1602ed2-1d3d-490e-aff3-9fe352e89639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in the dataset: 57997\n",
            "Number of unique words: 8860\n"
          ]
        }
      ],
      "source": [
        "def count_words(dataframe, column_name):\n",
        "    total_words = 0\n",
        "    unique_words = set()\n",
        "    for text in dataframe[column_name]:\n",
        "        words = text.split()\n",
        "        total_words += len(words)\n",
        "        unique_words.update(set(words))\n",
        "    return total_words, len(unique_words)\n",
        "\n",
        "# Usage:\n",
        "total_words, unique = count_words(updated_df, 'Content')\n",
        "\n",
        "print(f\"Total number of words in the dataset: {total_words}\")\n",
        "print(f\"Number of unique words: {unique}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-hLuQY3T_Pe"
      },
      "source": [
        "Last, by printing a few random samples of data, we can better observe how each entry looks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlTm10U3Tv-n",
        "outputId": "7cb12989-0c93-41d9-94db-be8ceef5cc32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content: poucas sensações se igualavam àquilo\n",
            "Content: ao entrar sentiu o cheiro da carcaça de carne que apodrecia ali dentro\n",
            "Content: parasita maníaco e solitário\n",
            "Content: seus olhos ficam molhados a garganta dói mas você não chora\n",
            "Content: e não existe nem uma remota chance de você sair andando daqui se me der de muito longe como resposta- venho diretamente do inferno seth- e me diga por que é que você está fugindo da sua esposa\n",
            "Content: já conseguisse passar algum dia da tua vida sem ter que apelar pra essa noia tua de esperança\n",
            "Content: eu posso sentir\n",
            "Content: faça-me um carinho e me dê um beijo depois\n",
            "Content: via-se dentro do próprio caixão\n",
            "Content: as origens confidenciadas a fat jack não muito tempo antes agora eram usadas contra si próprio\n"
          ]
        }
      ],
      "source": [
        "# Randomly select observations\n",
        "random_observations = updated_df.sample(n=10)\n",
        "\n",
        "# Print the content of the selected observations\n",
        "for idx, row in random_observations.iterrows():\n",
        "    print(f\"Content: {row['Content']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9VzdWMpXe5A"
      },
      "source": [
        "# Task Explanation and Data Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_uVP15dv-26"
      },
      "source": [
        "## Task explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzslqvK1w3g6"
      },
      "source": [
        "After years of writing, I want to have the experience of being a reader of myself. Of course, reading something I actually wrote would not be enough because I know what's coming next, which makes it impossible for me to feel surprise or enjoy the novelty of a text. This is why I want to train a model that can *write something new* like me.\n",
        "\n",
        "Technically speaking, this means training a model to understand the subtle patterns in my writing style to an extent that it is capable of generalizing them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj5YWhgI0Ru4"
      },
      "source": [
        "## Model selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eR_ZxWP5uXK"
      },
      "source": [
        "To accomplish this task, I chose to built a Long Short-Term Memory (LSTM) network. LSTMs are a type of RNN (Recurrent Neural Network) designed to capture long-term dependencies in sequential data.\n",
        "\n",
        "The way LSTMs work can be illustrated with the analogy of reading a book and trying to understand the plot: as we read the pages, we continuously update our understanding based on the current sentence and what we've read previously. LSTM does a similar process, but using numerical data instead of words. As in any neural network, each layer takes in some input, applies a set of weights, and produces an output. However, in an RNN (and consequently in an LSTM), there's a hidden state that's passed along from one step to the next. This hidden state acts like a memory, allowing the network to consider past information while processing current input. The difference between RNNs and LSTMs is that the latter is better at handling memory, suffering less from vanishing gradients when the input becomes large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgFcujF98A75"
      },
      "source": [
        "Each time a new input is given to the model, the following process happens:\n",
        "<br><br>\n",
        "\n",
        "#### **1) Deciding how much of the long-term memory to forget**\n",
        "The first part of LSTM (named Forget Gate) determines how much of the long term memory should be remembered for the current calculation. To do so, it uses the short-term memory ($h_{t-1}$) and the current input ($x_t$), returning a percentage $f_t$ that will be factored in the long-term memory later.\n",
        "\n",
        "$$\n",
        "f_t = \\sigma(W_{f} \\cdot [h_{t-1}, x_t] + b_f)\n",
        "$$\n",
        "\n",
        "in which any $W$ is a weight matrix and any $b$ is a bias vector.\n",
        "\n",
        "#### **2) Deciding what to add to the long-term memory**\n",
        "Next, the LSTM combines the short-term memory with the input to create a potential long-term memory, $C^\\sim_t$.\n",
        "\n",
        "$$\n",
        "C^\\sim_t = \\text{tanh}(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
        "$$\n",
        "\n",
        "\n",
        "Then, it determines what percentage of this potential memory should be actually incorporated into the long-term memory. This entire process happens on what is called the Input Gate.\n",
        "\n",
        "$$\n",
        "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
        "$$\n",
        "\n",
        "Following these steps, we update the long-term memory, $C_t$, based on the previous memory (and the amount of it we dediced to forget), and the candidate new memory (along with the amount we decided to remember):\n",
        "\n",
        "$$\n",
        "C_t = f_t \\star C_{t-1} + i_t \\star C^\\sim_t\n",
        "$$\n",
        "\n",
        "#### **3) Deciding what to output**\n",
        "\n",
        "Last, we output a value by first combining the short-term memory and the input, which gives us a candidate output $o_t$:\n",
        "\n",
        "$$\n",
        "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
        "$$\n",
        "\n",
        "And then we factor in our long-term memory, thus obtaining the final output. Given that this output will be the short-term memory for the next input, we call it $h_t$:\n",
        "\n",
        "$$\n",
        "h_t = o_t \\star \\text{tanh}(C_t)\n",
        "$$\n",
        "\n",
        "\n",
        "<br>\n",
        "<i>(Note: In a LSTM, everything we have just described is a single neuron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p6RhaXM8l0o"
      },
      "source": [
        "The weights and biases are randomly initialized and updated through backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIG1RKPTwCDZ"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzxD2z-Wxsj9"
      },
      "source": [
        "To create a model that can write like me, I am assuming that the piece category (poem, tale...) doesn't matter, which means all of the data can be grouped together. We thus start by converting all of the text into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zYf6xR2_LMk1"
      },
      "outputs": [],
      "source": [
        "raw_text = updated_df['Content'].str.cat(sep=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9TO1FpJx2fV"
      },
      "source": [
        "Next, we map the characters of the vocabulary to integers. Given that LSTMs are made to work with numerical data, each character in the text needs to be represented as a numerical value. This mapping allows us to process characters through the model, and later to reverse the process and convert numerical outputs into text again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSv-2VkRM5nJ",
        "outputId": "451d1850-a079-416d-8a4e-b19f0c1cd3fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Characters: 322745\n",
            "Total Vocab: 58\n",
            "Characters that compose the vocabulary: [' ', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', 'à', 'á', 'â', 'ã', 'ç', 'é', 'ê', 'í', 'ó', 'ô', 'õ', 'ú', '\\u200a', '–', '—', '’', '“', '”', '…']\n"
          ]
        }
      ],
      "source": [
        "# Create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters:\", n_chars)\n",
        "print(\"Total Vocab:\", n_vocab)\n",
        "print(f\"Characters that compose the vocabulary: {chars}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqbPirLm3PYf"
      },
      "source": [
        "Now, we split the data into inpt-output pairs. We want the model to predict one character at a time based on the previous 100 characters. Therefore, our input will be a sequence of 100 characters starting in $i$ and finishing in $i+99$, and the output, a sequence of 100 characters starting in $i+1$ and finishing in $i+100$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIBqxzYHNU2U",
        "outputId": "1b23172b-8a71-487c-db2f-7d21f63b82a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Patterns:  322645\n"
          ]
        }
      ],
      "source": [
        "# Prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        " seq_in = raw_text[i:i + seq_length]\n",
        " seq_out = raw_text[i + seq_length]\n",
        " dataX.append([char_to_int[char] for char in seq_in])\n",
        " dataY.append(char_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87jhqPPc44cl"
      },
      "source": [
        "Last, we reshape the input to the format expected by Keras, normalize it, and convert the output to 58-dimensional vectors (the size of the vocabulary). This means that, after processing the data, the LSTM will output a vector with probabilities for the next letter.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdCX17NHNexw"
      },
      "outputs": [],
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5D34hRDN-cM"
      },
      "source": [
        "# Model Initialization and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyHhOWMc8y6k"
      },
      "source": [
        "Below we initialize our LSTM model. We have two layers with 256 neurons each, two dropout layers in between to prevent overfitting, and a softmax at the end. Additionally, the fact we are using stacked LSTMs should also increase our capacity to represent more complex inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvB2WttwKZCb"
      },
      "outputs": [],
      "source": [
        "# Creates LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgo3xjUTvkfg"
      },
      "source": [
        "We then train the model for 70 epochs and a batch size of 60 (which means 60 training samples will be passed through the network before we update weights with backpropagation). With the resources from Colab Free, the training took 1h56min, achieving a minimum loss of 1.4405."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZNS__yDOLVR",
        "outputId": "6de22f81-bf90-4e4b-f865-3d620d993ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.9530\n",
            "Epoch 1: loss improved from inf to 1.95305, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-01-1.9530-bigger.hdf5\n",
            "5059/5059 [==============================] - 103s 20ms/step - loss: 1.9530\n",
            "Epoch 2/70\n",
            "   4/5059 [..............................] - ETA: 1:36 - loss: 1.8700"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.8884\n",
            "Epoch 2: loss improved from 1.95305 to 1.88847, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-02-1.8885-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 19ms/step - loss: 1.8885\n",
            "Epoch 3/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.8376\n",
            "Epoch 3: loss improved from 1.88847 to 1.83753, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-03-1.8375-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.8375\n",
            "Epoch 4/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.7971\n",
            "Epoch 4: loss improved from 1.83753 to 1.79718, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-04-1.7972-bigger.hdf5\n",
            "5059/5059 [==============================] - 98s 19ms/step - loss: 1.7972\n",
            "Epoch 5/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.7590\n",
            "Epoch 5: loss improved from 1.79718 to 1.75898, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-05-1.7590-bigger.hdf5\n",
            "5059/5059 [==============================] - 98s 19ms/step - loss: 1.7590\n",
            "Epoch 6/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.7299\n",
            "Epoch 6: loss improved from 1.75898 to 1.72987, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-06-1.7299-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 19ms/step - loss: 1.7299\n",
            "Epoch 7/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.7033\n",
            "Epoch 7: loss improved from 1.72987 to 1.70338, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-07-1.7034-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.7034\n",
            "Epoch 8/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.6775\n",
            "Epoch 8: loss improved from 1.70338 to 1.67749, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-08-1.6775-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.6775\n",
            "Epoch 9/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 2.8280\n",
            "Epoch 9: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 98s 19ms/step - loss: 2.8280\n",
            "Epoch 10/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 2.8474\n",
            "Epoch 10: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 98s 19ms/step - loss: 2.8474\n",
            "Epoch 11/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 2.7959\n",
            "Epoch 11: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 98s 19ms/step - loss: 2.7959\n",
            "Epoch 12/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 2.6421\n",
            "Epoch 12: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 2.6421\n",
            "Epoch 13/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 2.4720\n",
            "Epoch 13: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 2.4720\n",
            "Epoch 14/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 2.3062\n",
            "Epoch 14: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 98s 19ms/step - loss: 2.3062\n",
            "Epoch 15/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 2.1851\n",
            "Epoch 15: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 19ms/step - loss: 2.1851\n",
            "Epoch 16/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 2.0974\n",
            "Epoch 16: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 2.0974\n",
            "Epoch 17/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 2.0249\n",
            "Epoch 17: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 2.0249\n",
            "Epoch 18/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.9645\n",
            "Epoch 18: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 19ms/step - loss: 1.9645\n",
            "Epoch 19/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.9166\n",
            "Epoch 19: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 98s 19ms/step - loss: 1.9167\n",
            "Epoch 20/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.8775\n",
            "Epoch 20: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 98s 19ms/step - loss: 1.8775\n",
            "Epoch 21/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.8388\n",
            "Epoch 21: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 19ms/step - loss: 1.8388\n",
            "Epoch 22/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.8099\n",
            "Epoch 22: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 98s 19ms/step - loss: 1.8099\n",
            "Epoch 23/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.7832\n",
            "Epoch 23: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.7833\n",
            "Epoch 24/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.7584\n",
            "Epoch 24: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.7585\n",
            "Epoch 25/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.7373\n",
            "Epoch 25: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 19ms/step - loss: 1.7372\n",
            "Epoch 26/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.7145\n",
            "Epoch 26: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.7145\n",
            "Epoch 27/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.6975\n",
            "Epoch 27: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 19ms/step - loss: 1.6975\n",
            "Epoch 28/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.6797\n",
            "Epoch 28: loss did not improve from 1.67749\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.6798\n",
            "Epoch 29/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.6662\n",
            "Epoch 29: loss improved from 1.67749 to 1.66625, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-29-1.6663-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.6663\n",
            "Epoch 30/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.6502\n",
            "Epoch 30: loss improved from 1.66625 to 1.65019, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-30-1.6502-bigger.hdf5\n",
            "5059/5059 [==============================] - 100s 20ms/step - loss: 1.6502\n",
            "Epoch 31/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.6359\n",
            "Epoch 31: loss improved from 1.65019 to 1.63591, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-31-1.6359-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.6359\n",
            "Epoch 32/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.6240\n",
            "Epoch 32: loss improved from 1.63591 to 1.62398, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-32-1.6240-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.6240\n",
            "Epoch 33/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.6122\n",
            "Epoch 33: loss improved from 1.62398 to 1.61219, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-33-1.6122-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.6122\n",
            "Epoch 34/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.5998\n",
            "Epoch 34: loss improved from 1.61219 to 1.59982, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-34-1.5998-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5998\n",
            "Epoch 35/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.5903\n",
            "Epoch 35: loss improved from 1.59982 to 1.59032, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-35-1.5903-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5903\n",
            "Epoch 36/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.5823\n",
            "Epoch 36: loss improved from 1.59032 to 1.58237, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-36-1.5824-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5824\n",
            "Epoch 37/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.5705\n",
            "Epoch 37: loss improved from 1.58237 to 1.57046, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-37-1.5705-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5705\n",
            "Epoch 38/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.5622\n",
            "Epoch 38: loss improved from 1.57046 to 1.56221, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-38-1.5622-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5622\n",
            "Epoch 39/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.5533\n",
            "Epoch 39: loss improved from 1.56221 to 1.55335, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-39-1.5533-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5533\n",
            "Epoch 40/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.5446\n",
            "Epoch 40: loss improved from 1.55335 to 1.54461, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-40-1.5446-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5446\n",
            "Epoch 41/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.5391\n",
            "Epoch 41: loss improved from 1.54461 to 1.53905, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-41-1.5391-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5391\n",
            "Epoch 42/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.5308\n",
            "Epoch 42: loss improved from 1.53905 to 1.53078, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-42-1.5308-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5308\n",
            "Epoch 43/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.5258\n",
            "Epoch 43: loss improved from 1.53078 to 1.52581, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-43-1.5258-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5258\n",
            "Epoch 44/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.5202\n",
            "Epoch 44: loss improved from 1.52581 to 1.52023, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-44-1.5202-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5202\n",
            "Epoch 45/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.5656\n",
            "Epoch 45: loss did not improve from 1.52023\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5655\n",
            "Epoch 46/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.5063\n",
            "Epoch 46: loss improved from 1.52023 to 1.50640, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-46-1.5064-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5064\n",
            "Epoch 47/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.5018\n",
            "Epoch 47: loss improved from 1.50640 to 1.50177, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-47-1.5018-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5018\n",
            "Epoch 48/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.4944\n",
            "Epoch 48: loss improved from 1.50177 to 1.49433, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-48-1.4943-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4943\n",
            "Epoch 49/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.4883\n",
            "Epoch 49: loss improved from 1.49433 to 1.48826, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-49-1.4883-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4883\n",
            "Epoch 50/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.4839\n",
            "Epoch 50: loss improved from 1.48826 to 1.48391, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-50-1.4839-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4839\n",
            "Epoch 51/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.4777\n",
            "Epoch 51: loss improved from 1.48391 to 1.47773, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-51-1.4777-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4777\n",
            "Epoch 52/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.4732\n",
            "Epoch 52: loss improved from 1.47773 to 1.47311, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-52-1.4731-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 19ms/step - loss: 1.4731\n",
            "Epoch 53/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.4696\n",
            "Epoch 53: loss improved from 1.47311 to 1.46957, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-53-1.4696-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4696\n",
            "Epoch 54/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.4660\n",
            "Epoch 54: loss improved from 1.46957 to 1.46599, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-54-1.4660-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4660\n",
            "Epoch 55/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.4612\n",
            "Epoch 55: loss improved from 1.46599 to 1.46127, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-55-1.4613-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4613\n",
            "Epoch 56/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.4569\n",
            "Epoch 56: loss improved from 1.46127 to 1.45690, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-56-1.4569-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4569\n",
            "Epoch 57/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.5502\n",
            "Epoch 57: loss did not improve from 1.45690\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5502\n",
            "Epoch 58/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.4528\n",
            "Epoch 58: loss improved from 1.45690 to 1.45280, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-58-1.4528-bigger.hdf5\n",
            "5059/5059 [==============================] - 100s 20ms/step - loss: 1.4528\n",
            "Epoch 59/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.4518\n",
            "Epoch 59: loss improved from 1.45280 to 1.45198, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-59-1.4520-bigger.hdf5\n",
            "5059/5059 [==============================] - 100s 20ms/step - loss: 1.4520\n",
            "Epoch 60/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.4955\n",
            "Epoch 60: loss did not improve from 1.45198\n",
            "5059/5059 [==============================] - 100s 20ms/step - loss: 1.4955\n",
            "Epoch 61/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.4687\n",
            "Epoch 61: loss did not improve from 1.45198\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4687\n",
            "Epoch 62/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.4482\n",
            "Epoch 62: loss improved from 1.45198 to 1.44823, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-62-1.4482-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4482\n",
            "Epoch 63/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.4477\n",
            "Epoch 63: loss improved from 1.44823 to 1.44767, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-63-1.4477-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4477\n",
            "Epoch 64/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.5028\n",
            "Epoch 64: loss did not improve from 1.44767\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.5028\n",
            "Epoch 65/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.6423\n",
            "Epoch 65: loss did not improve from 1.44767\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.6423\n",
            "Epoch 66/70\n",
            "5059/5059 [==============================] - ETA: 0s - loss: 1.6089\n",
            "Epoch 66: loss did not improve from 1.44767\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.6089\n",
            "Epoch 67/70\n",
            "5058/5059 [============================>.] - ETA: 0s - loss: 1.4733\n",
            "Epoch 67: loss did not improve from 1.44767\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4732\n",
            "Epoch 68/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.4556\n",
            "Epoch 68: loss did not improve from 1.44767\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4557\n",
            "Epoch 69/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.4451\n",
            "Epoch 69: loss improved from 1.44767 to 1.44506, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-69-1.4451-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4451\n",
            "Epoch 70/70\n",
            "5057/5059 [============================>.] - ETA: 0s - loss: 1.4405\n",
            "Epoch 70: loss improved from 1.44506 to 1.44053, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/weights-improvement-70-1.4405-bigger.hdf5\n",
            "5059/5059 [==============================] - 99s 20ms/step - loss: 1.4405\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ca5293a58d0>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filepath = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/model/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=70, batch_size=60, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKWtz112BsaB"
      },
      "source": [
        "# Model Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHeqWi-SBu2P"
      },
      "source": [
        "With the model trained and its best version saved, we can now upload the weights and ask the model to generate new text based on an initial sample from the dataset. Below we have the functions for such."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhCkyy8AWHCg"
      },
      "outputs": [],
      "source": [
        "# load the network weights\n",
        "filename = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/model/weights-improvement-70-1.4405-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbhzK_aAPQVX"
      },
      "outputs": [],
      "source": [
        "# Similar to how we initialy converted letters to numbers, we now do the opposite proces\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2WDYprZPXFc"
      },
      "outputs": [],
      "source": [
        "def auto_generate():\n",
        "  # pick a random seed\n",
        "  start = np.random.randint(0, len(dataX)-1)\n",
        "  pattern = dataX[start]\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  print(\"\\nModel generation:\")\n",
        "\n",
        "  print_results(pattern)\n",
        "\n",
        "def custom_generate(text):\n",
        "  lst = []\n",
        "  lst.append([char_to_int[char] for char in text])\n",
        "  pattern = lst[0]\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  print(\"\\nModel generation:\")\n",
        "\n",
        "  print_results(pattern)\n",
        "\n",
        "\n",
        "def print_results(pattern):\n",
        "  # generate characters\n",
        "  for i in range(400):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "  print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCN3KlYPWmWT"
      },
      "source": [
        "## Quantitative performance metrics\n",
        "\n",
        "Unlike classification or regression tasks, there apparently are few metrics to evaluate the performance of generative models. Some of the most common ones are:\n",
        "\n",
        "1. **BLEU Score**: commonly used in translation tasks, it computes the similarity between the generated text and a set of reference (human-generated) texts.<br>\n",
        "2. **Perplexity**: measures how well a model predicts a sample of text.\n",
        "3. **ROGUE**: commonly used in text summarization, it evaluates the quality of summaries or generated text by measuring the overlap in n-grams (sequences of words) between the generated and the reference texts.\n",
        "\n",
        "None of these metrics seem to be applicable in our case, as our goal is to mimic the writing style of the original dataset while producing novel work (which is very difficult to evaluate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLUZ6nI0CWg4"
      },
      "source": [
        "## Qualitative performance metrics\n",
        "Given the lack of a quantitative metric, we will evaluate the model qualitatively, using the best possible evaluation method: the author's opinion of the model's output. The sampled seed and output from the next code cell are translated below. <br><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqGEP3aKGKSv",
        "outputId": "bc12e6d8-2a0f-408c-9488-8dc012e5274b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" brilhar teu raciocínio rápido esperto sempre fizera eu me esforçar para poder de alguma forma fazer  \"\n",
            "\n",
            "Model generation:\n",
            "a carne a lhe perfurar si próprio olhava buscando a lembrança como a cabeça do corpo de seu contexto de seu contexto que estava a conteguir a cada posta de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a c\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "auto_generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "118_J5GeZHnU"
      },
      "source": [
        "> Sampled seed text:\n",
        "\n",
        "> * \"[..] shine your smart quick thinking has always made me strive to be able to somehow do\"\n",
        "\n",
        "> Generated output:\n",
        "\n",
        "> - \"your flesh pierced himself looked at seeking the memory as the head of the body of his context of his context that he was comtaining at each slice of his post the first time the mind if for even more than the containment of his post the first time mind no matter how much the contention of its post the first time the mind no matter what the contention of its post the first time the mind no matter what the c\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0urCYULVBTl"
      },
      "source": [
        "**Evaluation:** while the seed text comes from a love poem I once wrote, the output seems to be a mix of words from a horror tale and generative hallucinations. There is also a lot of repetitiveness from the middle to the end of the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJf88OlbZU1Z"
      },
      "source": [
        "Next, we try customizing the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRcbEew4J4ia",
        "outputId": "d93d0654-4c00-406e-da87-aa0665fbbe6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" ontem foi um lindo dia o mar brilhava em olinda enquanto o sol iluminava as colinas e  \"\n",
            "\n",
            "Model generation:\n",
            "o context que estavam por causa de seu posto não se poderia estar e o context que estava a conteguir a cada posta de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a mente se por mais que a contenção de seu posto a primeira vez a \n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "custom_generate(\"ontem foi um lindo dia o mar brilhava em olinda enquanto o sol iluminava as colinas e \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7lMuOroNMHJ"
      },
      "source": [
        "> Custom seed:\n",
        "> - yesterday was a beautiful day, the sea shone in olinda while the sun illuminated the hills and\n",
        "\n",
        "> Output:\n",
        "> - the context they were in because of their position could not be and the context that they were containing at each position of their position the first time the mind no matter how much the contention of their position the first time the mind no matter what the containment of its post the first time the mind no matter how much more than the containment of its post the first time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zXlJ4I6V7Cs"
      },
      "source": [
        "Evaluation: while the input is about a beautiful day, the output seems to mix words from different pieces that do not make sense together. There is also a repetitive pattern in this output that resembles the repetition from the previous one, suggesting a bias from the model that will be in most outputs (I won't include extra outputs here, but in fact, it is)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srdpGZaea2Lu"
      },
      "source": [
        "# Discussion of Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrxGUctrZX9_"
      },
      "source": [
        "Overall, the LSTM is generating words correctly and hardly ever misspells any. This is, first of all, surprising, given that our model is simply predicting one character at a time. What we see is that it samples letters in a way that makes sense — for example, it doesn't sample a list of 20 consecutive letter, nor does it sample things like \"yzgsfat\" — and these samples turn out to have meaning to us, being words we actually understand. Additionally, it sometimes even presents words that could make sense together, such as \"because of their position\" or \"the mind becomes more than\".\n",
        "\n",
        "However, the sentences the LSTM builds are not really logical, and we are left with an output that resembles real language, but is not. One might argue that the performance could have been better if we had trained for longer, but most likely, it seems that there isn't much room for improvement with this LSTM. Some of the possibilities of exploration that can yield better outputs are training a model with more neurons, or maybe a model that predicts words, rather than characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ111xSPbbQN"
      },
      "source": [
        "# Executive Summary\n",
        "\n",
        "In this assignment, I explored the extent to which an LSTM can mock my way of writing. I started by importing the data from my last 10 years of writing, cleaning it, and preparing it to be processed by the LSTM. Then, I explained why LSTMs were a good choice, describing their advantage over RNNs and providing a step-by-step explanation of how they work. Next, I initialized my own LSTM and trained it on my dataset, keeping track of the loss and saving the weights of the best performing model. FInally, I used the trained model to make inferences on both samples from the dataset and customized inputs.\n",
        "\n",
        "Evaluating generative models like this seems to be a topic of debate on current research and, unlike with classification models, in which there are well-established metrics for performance evaluation, this does not seem to be the case here. Consequently, my analysis of its performance was based on a personal assessment of how closely the model's output resembled by own writing. While it was fascinating to see that the model could output words correctly, it was not capable of generating sentences that made sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNvTs2gbbdT6"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8FgYzHxcFA8"
      },
      "source": [
        "Tutorial for LSTM for text generation:\n",
        "- https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "How LSTMs work:\n",
        "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "- https://www.youtube.com/watch?v=YCzL96nL7j0\n",
        "\n",
        "How to evaluate generative models:\n",
        "- https://saturncloud.io/glossary/evaluating-generative-models/\n",
        "- https://arxiv.org/abs/2206.10935\n",
        "- ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCdJZ7cIDlWB"
      },
      "source": [
        "# Extra Section\n",
        "\n",
        "Given that the original model wasn't surprising, I decided to try a few alternatives and see how they perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5SPtvU71Thj"
      },
      "source": [
        "## 1) More neurons?\n",
        "\n",
        "What happens if we have the same network, but with more neurons? Theoretically, the greater number of neurons should allow the model to capture more information and patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTFSN5Gf4EbX"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS2yiuMJawrK"
      },
      "source": [
        "The model below trained for 20 epochs before Colab free shut itself down, achieving a minimum loss of 1.2908."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKiqCCmfa0R_"
      },
      "outputs": [],
      "source": [
        "# Creates LSTM model\n",
        "larger_model = Sequential()\n",
        "larger_model.add(LSTM(768, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "larger_model.add(Dropout(0.25))\n",
        "larger_model.add(LSTM(768))\n",
        "larger_model.add(Dropout(0.25))\n",
        "larger_model.add(Dense(y.shape[1], activation='softmax'))\n",
        "larger_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnhKxZtYbDoQ",
        "outputId": "e06618ec-91bf-4be2-95ef-99d219faf2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 2.8993\n",
            "Epoch 1: loss improved from inf to 2.89929, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-01-2.8993-bigger.hdf5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5378/5378 [==============================] - 566s 103ms/step - loss: 2.8993\n",
            "Epoch 2/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 2.8700\n",
            "Epoch 2: loss improved from 2.89929 to 2.87000, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-02-2.8700-bigger.hdf5\n",
            "5378/5378 [==============================] - 572s 106ms/step - loss: 2.8700\n",
            "Epoch 3/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 2.8361\n",
            "Epoch 3: loss improved from 2.87000 to 2.83611, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-03-2.8361-bigger.hdf5\n",
            "5378/5378 [==============================] - 572s 106ms/step - loss: 2.8361\n",
            "Epoch 4/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 2.7192\n",
            "Epoch 4: loss improved from 2.83611 to 2.71915, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-04-2.7192-bigger.hdf5\n",
            "5378/5378 [==============================] - 571s 106ms/step - loss: 2.7192\n",
            "Epoch 5/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 2.6117\n",
            "Epoch 5: loss improved from 2.71915 to 2.61171, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-05-2.6117-bigger.hdf5\n",
            "5378/5378 [==============================] - 574s 107ms/step - loss: 2.6117\n",
            "Epoch 6/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 2.4792\n",
            "Epoch 6: loss improved from 2.61171 to 2.47918, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-06-2.4792-bigger.hdf5\n",
            "5378/5378 [==============================] - 574s 107ms/step - loss: 2.4792\n",
            "Epoch 7/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 2.2653\n",
            "Epoch 7: loss improved from 2.47918 to 2.26529, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-07-2.2653-bigger.hdf5\n",
            "5378/5378 [==============================] - 574s 107ms/step - loss: 2.2653\n",
            "Epoch 8/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 2.0730\n",
            "Epoch 8: loss improved from 2.26529 to 2.07299, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-08-2.0730-bigger.hdf5\n",
            "5378/5378 [==============================] - 574s 107ms/step - loss: 2.0730\n",
            "Epoch 9/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.9308\n",
            "Epoch 9: loss improved from 2.07299 to 1.93078, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-09-1.9308-bigger.hdf5\n",
            "5378/5378 [==============================] - 574s 107ms/step - loss: 1.9308\n",
            "Epoch 10/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.8269\n",
            "Epoch 10: loss improved from 1.93078 to 1.82688, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-10-1.8269-bigger.hdf5\n",
            "5378/5378 [==============================] - 573s 107ms/step - loss: 1.8269\n",
            "Epoch 11/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.7365\n",
            "Epoch 11: loss improved from 1.82688 to 1.73645, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-11-1.7365-bigger.hdf5\n",
            "5378/5378 [==============================] - 572s 106ms/step - loss: 1.7365\n",
            "Epoch 12/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.6626\n",
            "Epoch 12: loss improved from 1.73645 to 1.66262, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-12-1.6626-bigger.hdf5\n",
            "5378/5378 [==============================] - 572s 106ms/step - loss: 1.6626\n",
            "Epoch 13/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.5997\n",
            "Epoch 13: loss improved from 1.66262 to 1.59973, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-13-1.5997-bigger.hdf5\n",
            "5378/5378 [==============================] - 573s 106ms/step - loss: 1.5997\n",
            "Epoch 14/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.5431\n",
            "Epoch 14: loss improved from 1.59973 to 1.54308, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-14-1.5431-bigger.hdf5\n",
            "5378/5378 [==============================] - 572s 106ms/step - loss: 1.5431\n",
            "Epoch 15/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.4904\n",
            "Epoch 15: loss improved from 1.54308 to 1.49038, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-15-1.4904-bigger.hdf5\n",
            "5378/5378 [==============================] - 571s 106ms/step - loss: 1.4904\n",
            "Epoch 16/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.4414\n",
            "Epoch 16: loss improved from 1.49038 to 1.44145, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-16-1.4414-bigger.hdf5\n",
            "5378/5378 [==============================] - 571s 106ms/step - loss: 1.4414\n",
            "Epoch 17/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.3984\n",
            "Epoch 17: loss improved from 1.44145 to 1.39843, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-17-1.3984-bigger.hdf5\n",
            "5378/5378 [==============================] - 572s 106ms/step - loss: 1.3984\n",
            "Epoch 18/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.3601\n",
            "Epoch 18: loss improved from 1.39843 to 1.36015, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-18-1.3601-bigger.hdf5\n",
            "5378/5378 [==============================] - 572s 106ms/step - loss: 1.3601\n",
            "Epoch 19/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.3266\n",
            "Epoch 19: loss improved from 1.36015 to 1.32662, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-19-1.3266-bigger.hdf5\n",
            "5378/5378 [==============================] - 571s 106ms/step - loss: 1.3266\n",
            "Epoch 20/70\n",
            "5378/5378 [==============================] - ETA: 0s - loss: 1.2908\n",
            "Epoch 20: loss improved from 1.32662 to 1.29084, saving model to /content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-20-1.2908-bigger.hdf5\n",
            "5378/5378 [==============================] - 572s 106ms/step - loss: 1.2908\n",
            "Epoch 21/70\n",
            "1416/5378 [======>.......................] - ETA: 7:00 - loss: 1.2366"
          ]
        }
      ],
      "source": [
        "filepath = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# fit the model\n",
        "larger_model.fit(X, y, epochs=70, batch_size=60, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnRV2r1p17kQ"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRQhOkrb2CFH"
      },
      "outputs": [],
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "filename_larger = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/larger_model/weights-improvement-20-1.2908-bigger.hdf5\"\n",
        "larger_model.load_weights(filename_larger)\n",
        "larger_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "\n",
        "def auto_generate():\n",
        "  # pick a random seed\n",
        "  start = np.random.randint(0, len(dataX)-1)\n",
        "  pattern = dataX[start]\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  print(\"\\nModel generation:\")\n",
        "\n",
        "  print_results(pattern)\n",
        "\n",
        "def custom_generate(text):\n",
        "  lst = []\n",
        "  lst.append([char_to_int[char] for char in text])\n",
        "  pattern = lst[0]\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  print(\"\\nModel generation:\")\n",
        "\n",
        "  print_results(pattern)\n",
        "\n",
        "\n",
        "def print_results(pattern):\n",
        "  # generate characters\n",
        "  for i in range(300):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = larger_model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "  print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESTkQl012VND",
        "outputId": "2f74fc1e-7ffa-4fde-e1a5-947a5ab9544e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" pode encontrar o código desse meu aplicativo aqui enfim terminamos espero muito que o artigo tenha s \"\n",
            "\n",
            "Model generation:\n",
            "ido de madeira ertava entrar no casal para o contrabandista não era mais puro srabalho de seu app e a menos de se alguma manhira pue estava em seu rosto de tm celes  fmi a única coisa que estava en cada novo trabalho na parte de corrado para o casal para o contrabandista não era mais puro srabalho d\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "auto_generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn4riEw_5qLY"
      },
      "source": [
        "> Sampled seed text:\n",
        "\n",
        "> * \"you can find the code for my application here finally we're done I really hope this article has s\"\n",
        "\n",
        "> Generated output:\n",
        "\n",
        "> - \"gone from wood to eter the couple for the smuggler was no longer pure swork of his app and unless some maner pue was in his face of tm celes fmi the only thing that was inn each new work in the part of corrado for the couple For the smuggler it was no longer pure work of art.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUpaWJ5D2XLu",
        "outputId": "3e929b5e-c95e-4ef0-d332-9479fa3673b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" ontem foi um lindo dia o mar brilhava em olinda enquanto o sol iluminava as colinas e  \"\n",
            "\n",
            "Model generation:\n",
            "a porta de seu app e a menos de se alguma manhira pue estava em seu rosto de tm celes  fme souris por completo perceber que o cara eez o contrabandista não era mais puro srabalho de seu app e a menos de se alguma manhira pue estava em seu rosto de tm celes  fme souris por completo perceber que o car\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "custom_generate(\"ontem foi um lindo dia o mar brilhava em olinda enquanto o sol iluminava as colinas e \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCxWUiMa6b-p"
      },
      "source": [
        "> Sampled seed text:\n",
        "\n",
        "> * \"yesterday was a beautiful day the sea shone in olinda while the sun illuminated the hills and\"\n",
        "\n",
        "> Generated output:\n",
        "\n",
        "> - \"the door or your app and unless of some maner pue was in your face of wit celes fme souris completely realizing the guy iid the smuggler wasn't more pure swork of your app and unless that some maner pue was in your face wit celes fme  souris completely realize the guy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8KqmeHU7SO4"
      },
      "source": [
        "### Analysis of results\n",
        "\n",
        "This model performed worst than the previous one. It is capable of outputting characters in sequences that resemble words (placing spaces correctly, alternating between vowels and consonants...), but it often makes grammatical mistakes. Furthermore, the words it generates correctly do not make sense — unlike the previous model, neighboring words have little connection with each other.\n",
        "\n",
        "Last, during training, the loss improved very little from epoch to epoch, suggesting that longer training times would not contribute much for improving the quality of the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-wsylCW14l1"
      },
      "source": [
        "## 2) Words instead of characters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i053RntGhHbI"
      },
      "source": [
        "Another possibility is adjusting the model to predict words, instead of characters. Although this increases considerably the number of different inputs the model can take (and patterns it needs to learn), it might make it easier for the model to connect words together in a coherent way.\n",
        "\n",
        "To make it work, I had to change the pipeline that processed the text, which resulted in the one below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SZFv4EQ7ZqG"
      },
      "source": [
        "### Data processing and model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE867FwVhLNo",
        "outputId": "93423e5a-2b6d-443e-b189-da66731fc24f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Sequences:  58233\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Concatenate text data\n",
        "raw_text = updated_df['Content'].str.cat(sep=' ')\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([raw_text])\n",
        "sequences = tokenizer.texts_to_sequences([raw_text])[0]\n",
        "total_words = len(tokenizer.word_index) + 1  # Adding 1 for Out of Vocabulary (OOV) token\n",
        "\n",
        "# Prepare sequences of 30 words as input and one word as output\n",
        "seq_length = 30\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(seq_length, len(sequences)):\n",
        "    seq_in = sequences[i - seq_length:i]\n",
        "    seq_out = sequences[i]\n",
        "    dataX.append(seq_in)\n",
        "    dataY.append(seq_out)\n",
        "\n",
        "# Convert the sequences into numpy arrays\n",
        "X = np.array(dataX)\n",
        "y = to_categorical(dataY, num_classes=total_words)\n",
        "\n",
        "print(\"Total Sequences: \", len(dataX))\n",
        "\n",
        "# Now, X contains sequences of 30 words, and y is the one-hot encoded output.\n",
        "# These can be used for training the LSTM model.\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "n_patterns = len(dataX)\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(total_words)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoFKK0KA65yQ"
      },
      "source": [
        "I then defined a model that was identical to the first one and trained it for 3 hours, covering 300 epochs with a batch size of 15."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMLovNwxhePg"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "# Adjusting the model for word-level prediction\n",
        "words_model = Sequential()\n",
        "words_model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "words_model.add(Dropout(0.2))\n",
        "words_model.add(LSTM(256))  # No return_sequences needed in the last LSTM layer\n",
        "words_model.add(Dropout(0.2))\n",
        "words_model.add(Dense(total_words, activation='softmax'))  # Changed y.shape[1] to total_words\n",
        "words_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "filepath = \"/content/drive/MyDrive/156 materials/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_8xSA0-iy_g",
        "outputId": "e42509d1-97ea-41f5-9d95-75bc1d5ec2c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 7.2208\n",
            "Epoch 1: loss improved from inf to 7.22084, saving model to /content/drive/MyDrive/156 materials/weights-improvement-01-7.2208-bigger.hdf5\n",
            "3883/3883 [==============================] - 39s 9ms/step - loss: 7.2208\n",
            "Epoch 2/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 7.0431\n",
            "Epoch 2: loss improved from 7.22084 to 7.04317, saving model to /content/drive/MyDrive/156 materials/weights-improvement-02-7.0432-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 7.0432\n",
            "Epoch 3/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 7.0139\n",
            "Epoch 3: loss improved from 7.04317 to 7.01351, saving model to /content/drive/MyDrive/156 materials/weights-improvement-03-7.0135-bigger.hdf5\n",
            "3883/3883 [==============================] - 33s 9ms/step - loss: 7.0135\n",
            "Epoch 4/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 6.9937\n",
            "Epoch 4: loss improved from 7.01351 to 6.99408, saving model to /content/drive/MyDrive/156 materials/weights-improvement-04-6.9941-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.9941\n",
            "Epoch 5/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 6.9686\n",
            "Epoch 5: loss improved from 6.99408 to 6.96856, saving model to /content/drive/MyDrive/156 materials/weights-improvement-05-6.9686-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.9686\n",
            "Epoch 6/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 6.9305\n",
            "Epoch 6: loss improved from 6.96856 to 6.93041, saving model to /content/drive/MyDrive/156 materials/weights-improvement-06-6.9304-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.9304\n",
            "Epoch 7/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 6.8831\n",
            "Epoch 7: loss improved from 6.93041 to 6.88298, saving model to /content/drive/MyDrive/156 materials/weights-improvement-07-6.8830-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.8830\n",
            "Epoch 8/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 6.8353\n",
            "Epoch 8: loss improved from 6.88298 to 6.83534, saving model to /content/drive/MyDrive/156 materials/weights-improvement-08-6.8353-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.8353\n",
            "Epoch 9/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 6.7707\n",
            "Epoch 9: loss improved from 6.83534 to 6.77090, saving model to /content/drive/MyDrive/156 materials/weights-improvement-09-6.7709-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.7709\n",
            "Epoch 10/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 6.7005\n",
            "Epoch 10: loss improved from 6.77090 to 6.70074, saving model to /content/drive/MyDrive/156 materials/weights-improvement-10-6.7007-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.7007\n",
            "Epoch 11/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 6.6376\n",
            "Epoch 11: loss improved from 6.70074 to 6.63761, saving model to /content/drive/MyDrive/156 materials/weights-improvement-11-6.6376-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.6376\n",
            "Epoch 12/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 6.5746\n",
            "Epoch 12: loss improved from 6.63761 to 6.57470, saving model to /content/drive/MyDrive/156 materials/weights-improvement-12-6.5747-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.5747\n",
            "Epoch 13/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 6.4990\n",
            "Epoch 13: loss improved from 6.57470 to 6.49927, saving model to /content/drive/MyDrive/156 materials/weights-improvement-13-6.4993-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.4993\n",
            "Epoch 14/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 6.4160\n",
            "Epoch 14: loss improved from 6.49927 to 6.41628, saving model to /content/drive/MyDrive/156 materials/weights-improvement-14-6.4163-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.4163\n",
            "Epoch 15/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 6.3210\n",
            "Epoch 15: loss improved from 6.41628 to 6.32108, saving model to /content/drive/MyDrive/156 materials/weights-improvement-15-6.3211-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.3211\n",
            "Epoch 16/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 6.2090\n",
            "Epoch 16: loss improved from 6.32108 to 6.20902, saving model to /content/drive/MyDrive/156 materials/weights-improvement-16-6.2090-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.2090\n",
            "Epoch 17/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 6.1023\n",
            "Epoch 17: loss improved from 6.20902 to 6.10249, saving model to /content/drive/MyDrive/156 materials/weights-improvement-17-6.1025-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 6.1025\n",
            "Epoch 18/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 5.9897\n",
            "Epoch 18: loss improved from 6.10249 to 5.98971, saving model to /content/drive/MyDrive/156 materials/weights-improvement-18-5.9897-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 5.9897\n",
            "Epoch 19/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 5.8788\n",
            "Epoch 19: loss improved from 5.98971 to 5.87880, saving model to /content/drive/MyDrive/156 materials/weights-improvement-19-5.8788-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 5.8788\n",
            "Epoch 20/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 5.7734\n",
            "Epoch 20: loss improved from 5.87880 to 5.77338, saving model to /content/drive/MyDrive/156 materials/weights-improvement-20-5.7734-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 5.7734\n",
            "Epoch 21/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 5.6739\n",
            "Epoch 21: loss improved from 5.77338 to 5.67406, saving model to /content/drive/MyDrive/156 materials/weights-improvement-21-5.6741-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 5.6741\n",
            "Epoch 22/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 5.5787\n",
            "Epoch 22: loss improved from 5.67406 to 5.57896, saving model to /content/drive/MyDrive/156 materials/weights-improvement-22-5.5790-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 5.5790\n",
            "Epoch 23/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 5.4746\n",
            "Epoch 23: loss improved from 5.57896 to 5.47398, saving model to /content/drive/MyDrive/156 materials/weights-improvement-23-5.4740-bigger.hdf5\n",
            "3883/3883 [==============================] - 33s 9ms/step - loss: 5.4740\n",
            "Epoch 24/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 5.3786\n",
            "Epoch 24: loss improved from 5.47398 to 5.37856, saving model to /content/drive/MyDrive/156 materials/weights-improvement-24-5.3786-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 5.3786\n",
            "Epoch 25/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 5.2754\n",
            "Epoch 25: loss improved from 5.37856 to 5.27547, saving model to /content/drive/MyDrive/156 materials/weights-improvement-25-5.2755-bigger.hdf5\n",
            "3883/3883 [==============================] - 33s 9ms/step - loss: 5.2755\n",
            "Epoch 26/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 5.1683\n",
            "Epoch 26: loss improved from 5.27547 to 5.16833, saving model to /content/drive/MyDrive/156 materials/weights-improvement-26-5.1683-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 5.1683\n",
            "Epoch 27/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 5.0645\n",
            "Epoch 27: loss improved from 5.16833 to 5.06448, saving model to /content/drive/MyDrive/156 materials/weights-improvement-27-5.0645-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 5.0645\n",
            "Epoch 28/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 4.9501\n",
            "Epoch 28: loss improved from 5.06448 to 4.95011, saving model to /content/drive/MyDrive/156 materials/weights-improvement-28-4.9501-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 4.9501\n",
            "Epoch 29/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 4.8461\n",
            "Epoch 29: loss improved from 4.95011 to 4.84631, saving model to /content/drive/MyDrive/156 materials/weights-improvement-29-4.8463-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 4.8463\n",
            "Epoch 30/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 4.7393\n",
            "Epoch 30: loss improved from 4.84631 to 4.73983, saving model to /content/drive/MyDrive/156 materials/weights-improvement-30-4.7398-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 4.7398\n",
            "Epoch 31/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 4.6311\n",
            "Epoch 31: loss improved from 4.73983 to 4.63157, saving model to /content/drive/MyDrive/156 materials/weights-improvement-31-4.6316-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 4.6316\n",
            "Epoch 32/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 4.5278\n",
            "Epoch 32: loss improved from 4.63157 to 4.52785, saving model to /content/drive/MyDrive/156 materials/weights-improvement-32-4.5279-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 4.5279\n",
            "Epoch 33/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 4.4212\n",
            "Epoch 33: loss improved from 4.52785 to 4.42113, saving model to /content/drive/MyDrive/156 materials/weights-improvement-33-4.4211-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 4.4211\n",
            "Epoch 34/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 4.3171\n",
            "Epoch 34: loss improved from 4.42113 to 4.31725, saving model to /content/drive/MyDrive/156 materials/weights-improvement-34-4.3173-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 4.3173\n",
            "Epoch 35/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 4.2309\n",
            "Epoch 35: loss improved from 4.31725 to 4.23087, saving model to /content/drive/MyDrive/156 materials/weights-improvement-35-4.2309-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 4.2309\n",
            "Epoch 36/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 4.1265\n",
            "Epoch 36: loss improved from 4.23087 to 4.12650, saving model to /content/drive/MyDrive/156 materials/weights-improvement-36-4.1265-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 4.1265\n",
            "Epoch 37/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 4.0360\n",
            "Epoch 37: loss improved from 4.12650 to 4.03622, saving model to /content/drive/MyDrive/156 materials/weights-improvement-37-4.0362-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 4.0362\n",
            "Epoch 38/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 3.9421\n",
            "Epoch 38: loss improved from 4.03622 to 3.94211, saving model to /content/drive/MyDrive/156 materials/weights-improvement-38-3.9421-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.9421\n",
            "Epoch 39/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 3.8605\n",
            "Epoch 39: loss improved from 3.94211 to 3.86049, saving model to /content/drive/MyDrive/156 materials/weights-improvement-39-3.8605-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 3.8605\n",
            "Epoch 40/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 3.7689\n",
            "Epoch 40: loss improved from 3.86049 to 3.76944, saving model to /content/drive/MyDrive/156 materials/weights-improvement-40-3.7694-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.7694\n",
            "Epoch 41/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 3.6972\n",
            "Epoch 41: loss improved from 3.76944 to 3.69727, saving model to /content/drive/MyDrive/156 materials/weights-improvement-41-3.6973-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.6973\n",
            "Epoch 42/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 3.6053\n",
            "Epoch 42: loss improved from 3.69727 to 3.60529, saving model to /content/drive/MyDrive/156 materials/weights-improvement-42-3.6053-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.6053\n",
            "Epoch 43/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 3.5258\n",
            "Epoch 43: loss improved from 3.60529 to 3.52558, saving model to /content/drive/MyDrive/156 materials/weights-improvement-43-3.5256-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.5256\n",
            "Epoch 44/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 3.4458\n",
            "Epoch 44: loss improved from 3.52558 to 3.44605, saving model to /content/drive/MyDrive/156 materials/weights-improvement-44-3.4461-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.4461\n",
            "Epoch 45/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 3.3838\n",
            "Epoch 45: loss improved from 3.44605 to 3.38359, saving model to /content/drive/MyDrive/156 materials/weights-improvement-45-3.3836-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.3836\n",
            "Epoch 46/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 3.3107\n",
            "Epoch 46: loss improved from 3.38359 to 3.31074, saving model to /content/drive/MyDrive/156 materials/weights-improvement-46-3.3107-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.3107\n",
            "Epoch 47/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 3.2448\n",
            "Epoch 47: loss improved from 3.31074 to 3.24479, saving model to /content/drive/MyDrive/156 materials/weights-improvement-47-3.2448-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.2448\n",
            "Epoch 48/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 3.1864\n",
            "Epoch 48: loss improved from 3.24479 to 3.18641, saving model to /content/drive/MyDrive/156 materials/weights-improvement-48-3.1864-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.1864\n",
            "Epoch 49/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 3.1173\n",
            "Epoch 49: loss improved from 3.18641 to 3.11701, saving model to /content/drive/MyDrive/156 materials/weights-improvement-49-3.1170-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.1170\n",
            "Epoch 50/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 3.0664\n",
            "Epoch 50: loss improved from 3.11701 to 3.06631, saving model to /content/drive/MyDrive/156 materials/weights-improvement-50-3.0663-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 3.0663\n",
            "Epoch 51/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 3.0022\n",
            "Epoch 51: loss improved from 3.06631 to 3.00224, saving model to /content/drive/MyDrive/156 materials/weights-improvement-51-3.0022-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 3.0022\n",
            "Epoch 52/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 2.9541\n",
            "Epoch 52: loss improved from 3.00224 to 2.95413, saving model to /content/drive/MyDrive/156 materials/weights-improvement-52-2.9541-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 2.9541\n",
            "Epoch 53/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 2.9097\n",
            "Epoch 53: loss improved from 2.95413 to 2.91015, saving model to /content/drive/MyDrive/156 materials/weights-improvement-53-2.9102-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.9102\n",
            "Epoch 54/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 2.8495\n",
            "Epoch 54: loss improved from 2.91015 to 2.84942, saving model to /content/drive/MyDrive/156 materials/weights-improvement-54-2.8494-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.8494\n",
            "Epoch 55/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 2.7955\n",
            "Epoch 55: loss improved from 2.84942 to 2.79564, saving model to /content/drive/MyDrive/156 materials/weights-improvement-55-2.7956-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.7956\n",
            "Epoch 56/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 2.7506\n",
            "Epoch 56: loss improved from 2.79564 to 2.75062, saving model to /content/drive/MyDrive/156 materials/weights-improvement-56-2.7506-bigger.hdf5\n",
            "3883/3883 [==============================] - 33s 9ms/step - loss: 2.7506\n",
            "Epoch 57/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 2.7100\n",
            "Epoch 57: loss improved from 2.75062 to 2.70996, saving model to /content/drive/MyDrive/156 materials/weights-improvement-57-2.7100-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.7100\n",
            "Epoch 58/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 2.6658\n",
            "Epoch 58: loss improved from 2.70996 to 2.66628, saving model to /content/drive/MyDrive/156 materials/weights-improvement-58-2.6663-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.6663\n",
            "Epoch 59/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 2.6145\n",
            "Epoch 59: loss improved from 2.66628 to 2.61448, saving model to /content/drive/MyDrive/156 materials/weights-improvement-59-2.6145-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.6145\n",
            "Epoch 60/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 2.5782\n",
            "Epoch 60: loss improved from 2.61448 to 2.57823, saving model to /content/drive/MyDrive/156 materials/weights-improvement-60-2.5782-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.5782\n",
            "Epoch 61/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 2.5296\n",
            "Epoch 61: loss improved from 2.57823 to 2.53002, saving model to /content/drive/MyDrive/156 materials/weights-improvement-61-2.5300-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.5300\n",
            "Epoch 62/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 2.4899\n",
            "Epoch 62: loss improved from 2.53002 to 2.48989, saving model to /content/drive/MyDrive/156 materials/weights-improvement-62-2.4899-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.4899\n",
            "Epoch 63/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 2.4647\n",
            "Epoch 63: loss improved from 2.48989 to 2.46486, saving model to /content/drive/MyDrive/156 materials/weights-improvement-63-2.4649-bigger.hdf5\n",
            "3883/3883 [==============================] - 33s 9ms/step - loss: 2.4649\n",
            "Epoch 64/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 2.4351\n",
            "Epoch 64: loss improved from 2.46486 to 2.43546, saving model to /content/drive/MyDrive/156 materials/weights-improvement-64-2.4355-bigger.hdf5\n",
            "3883/3883 [==============================] - 33s 9ms/step - loss: 2.4355\n",
            "Epoch 65/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 2.4031\n",
            "Epoch 65: loss improved from 2.43546 to 2.40307, saving model to /content/drive/MyDrive/156 materials/weights-improvement-65-2.4031-bigger.hdf5\n",
            "3883/3883 [==============================] - 33s 9ms/step - loss: 2.4031\n",
            "Epoch 66/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 2.3630\n",
            "Epoch 66: loss improved from 2.40307 to 2.36296, saving model to /content/drive/MyDrive/156 materials/weights-improvement-66-2.3630-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.3630\n",
            "Epoch 67/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 2.3330\n",
            "Epoch 67: loss improved from 2.36296 to 2.33293, saving model to /content/drive/MyDrive/156 materials/weights-improvement-67-2.3329-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.3329\n",
            "Epoch 68/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 2.2962\n",
            "Epoch 68: loss improved from 2.33293 to 2.29619, saving model to /content/drive/MyDrive/156 materials/weights-improvement-68-2.2962-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.2962\n",
            "Epoch 69/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 2.2636\n",
            "Epoch 69: loss improved from 2.29619 to 2.26358, saving model to /content/drive/MyDrive/156 materials/weights-improvement-69-2.2636-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.2636\n",
            "Epoch 70/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 2.2376\n",
            "Epoch 70: loss improved from 2.26358 to 2.23769, saving model to /content/drive/MyDrive/156 materials/weights-improvement-70-2.2377-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 2.2377\n",
            "Epoch 71/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 2.2120\n",
            "Epoch 71: loss improved from 2.23769 to 2.21204, saving model to /content/drive/MyDrive/156 materials/weights-improvement-71-2.2120-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 2.2120\n",
            "Epoch 72/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 2.1810\n",
            "Epoch 72: loss improved from 2.21204 to 2.18070, saving model to /content/drive/MyDrive/156 materials/weights-improvement-72-2.1807-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.1807\n",
            "Epoch 73/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 2.1503\n",
            "Epoch 73: loss improved from 2.18070 to 2.15028, saving model to /content/drive/MyDrive/156 materials/weights-improvement-73-2.1503-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 2.1503\n",
            "Epoch 74/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 2.1193\n",
            "Epoch 74: loss improved from 2.15028 to 2.11927, saving model to /content/drive/MyDrive/156 materials/weights-improvement-74-2.1193-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 2.1193\n",
            "Epoch 75/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 2.0956\n",
            "Epoch 75: loss improved from 2.11927 to 2.09559, saving model to /content/drive/MyDrive/156 materials/weights-improvement-75-2.0956-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 2.0956\n",
            "Epoch 76/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 2.0678\n",
            "Epoch 76: loss improved from 2.09559 to 2.06800, saving model to /content/drive/MyDrive/156 materials/weights-improvement-76-2.0680-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.0680\n",
            "Epoch 77/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 2.0363\n",
            "Epoch 77: loss improved from 2.06800 to 2.03663, saving model to /content/drive/MyDrive/156 materials/weights-improvement-77-2.0366-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 2.0366\n",
            "Epoch 78/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 2.0227\n",
            "Epoch 78: loss improved from 2.03663 to 2.02282, saving model to /content/drive/MyDrive/156 materials/weights-improvement-78-2.0228-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 2.0228\n",
            "Epoch 79/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 1.9984\n",
            "Epoch 79: loss improved from 2.02282 to 1.99846, saving model to /content/drive/MyDrive/156 materials/weights-improvement-79-1.9985-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.9985\n",
            "Epoch 80/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 1.9735\n",
            "Epoch 80: loss improved from 1.99846 to 1.97346, saving model to /content/drive/MyDrive/156 materials/weights-improvement-80-1.9735-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.9735\n",
            "Epoch 81/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.9583\n",
            "Epoch 81: loss improved from 1.97346 to 1.95811, saving model to /content/drive/MyDrive/156 materials/weights-improvement-81-1.9581-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.9581\n",
            "Epoch 82/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 1.9363\n",
            "Epoch 82: loss improved from 1.95811 to 1.93666, saving model to /content/drive/MyDrive/156 materials/weights-improvement-82-1.9367-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.9367\n",
            "Epoch 83/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.9054\n",
            "Epoch 83: loss improved from 1.93666 to 1.90555, saving model to /content/drive/MyDrive/156 materials/weights-improvement-83-1.9056-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.9056\n",
            "Epoch 84/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.8878\n",
            "Epoch 84: loss improved from 1.90555 to 1.88783, saving model to /content/drive/MyDrive/156 materials/weights-improvement-84-1.8878-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.8878\n",
            "Epoch 85/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 1.8722\n",
            "Epoch 85: loss improved from 1.88783 to 1.87224, saving model to /content/drive/MyDrive/156 materials/weights-improvement-85-1.8722-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.8722\n",
            "Epoch 86/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.8424\n",
            "Epoch 86: loss improved from 1.87224 to 1.84261, saving model to /content/drive/MyDrive/156 materials/weights-improvement-86-1.8426-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.8426\n",
            "Epoch 87/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.8271\n",
            "Epoch 87: loss improved from 1.84261 to 1.82739, saving model to /content/drive/MyDrive/156 materials/weights-improvement-87-1.8274-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.8274\n",
            "Epoch 88/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.8076\n",
            "Epoch 88: loss improved from 1.82739 to 1.80763, saving model to /content/drive/MyDrive/156 materials/weights-improvement-88-1.8076-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.8076\n",
            "Epoch 89/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.7851\n",
            "Epoch 89: loss improved from 1.80763 to 1.78519, saving model to /content/drive/MyDrive/156 materials/weights-improvement-89-1.7852-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.7852\n",
            "Epoch 90/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.7601\n",
            "Epoch 90: loss improved from 1.78519 to 1.76051, saving model to /content/drive/MyDrive/156 materials/weights-improvement-90-1.7605-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.7605\n",
            "Epoch 91/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.7460\n",
            "Epoch 91: loss improved from 1.76051 to 1.74594, saving model to /content/drive/MyDrive/156 materials/weights-improvement-91-1.7459-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.7459\n",
            "Epoch 92/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.7246\n",
            "Epoch 92: loss improved from 1.74594 to 1.72466, saving model to /content/drive/MyDrive/156 materials/weights-improvement-92-1.7247-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.7247\n",
            "Epoch 93/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.7085\n",
            "Epoch 93: loss improved from 1.72466 to 1.70848, saving model to /content/drive/MyDrive/156 materials/weights-improvement-93-1.7085-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.7085\n",
            "Epoch 94/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.6954\n",
            "Epoch 94: loss improved from 1.70848 to 1.69550, saving model to /content/drive/MyDrive/156 materials/weights-improvement-94-1.6955-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.6955\n",
            "Epoch 95/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.6922\n",
            "Epoch 95: loss improved from 1.69550 to 1.69217, saving model to /content/drive/MyDrive/156 materials/weights-improvement-95-1.6922-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.6922\n",
            "Epoch 96/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.6581\n",
            "Epoch 96: loss improved from 1.69217 to 1.65807, saving model to /content/drive/MyDrive/156 materials/weights-improvement-96-1.6581-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.6581\n",
            "Epoch 97/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 1.6564\n",
            "Epoch 97: loss improved from 1.65807 to 1.65662, saving model to /content/drive/MyDrive/156 materials/weights-improvement-97-1.6566-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.6566\n",
            "Epoch 98/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 1.6265\n",
            "Epoch 98: loss improved from 1.65662 to 1.62651, saving model to /content/drive/MyDrive/156 materials/weights-improvement-98-1.6265-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.6265\n",
            "Epoch 99/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.6070\n",
            "Epoch 99: loss improved from 1.62651 to 1.60691, saving model to /content/drive/MyDrive/156 materials/weights-improvement-99-1.6069-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.6069\n",
            "Epoch 100/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.6018\n",
            "Epoch 100: loss improved from 1.60691 to 1.60200, saving model to /content/drive/MyDrive/156 materials/weights-improvement-100-1.6020-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.6020\n",
            "Epoch 101/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.5697\n",
            "Epoch 101: loss improved from 1.60200 to 1.56969, saving model to /content/drive/MyDrive/156 materials/weights-improvement-101-1.5697-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.5697\n",
            "Epoch 102/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.5639\n",
            "Epoch 102: loss improved from 1.56969 to 1.56403, saving model to /content/drive/MyDrive/156 materials/weights-improvement-102-1.5640-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.5640\n",
            "Epoch 103/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.5535\n",
            "Epoch 103: loss improved from 1.56403 to 1.55344, saving model to /content/drive/MyDrive/156 materials/weights-improvement-103-1.5534-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.5534\n",
            "Epoch 104/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.5385\n",
            "Epoch 104: loss improved from 1.55344 to 1.53853, saving model to /content/drive/MyDrive/156 materials/weights-improvement-104-1.5385-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.5385\n",
            "Epoch 105/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 1.5259\n",
            "Epoch 105: loss improved from 1.53853 to 1.52633, saving model to /content/drive/MyDrive/156 materials/weights-improvement-105-1.5263-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.5263\n",
            "Epoch 106/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.4996\n",
            "Epoch 106: loss improved from 1.52633 to 1.49960, saving model to /content/drive/MyDrive/156 materials/weights-improvement-106-1.4996-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.4996\n",
            "Epoch 107/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 1.4865\n",
            "Epoch 107: loss improved from 1.49960 to 1.48654, saving model to /content/drive/MyDrive/156 materials/weights-improvement-107-1.4865-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.4865\n",
            "Epoch 108/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.4820\n",
            "Epoch 108: loss improved from 1.48654 to 1.48208, saving model to /content/drive/MyDrive/156 materials/weights-improvement-108-1.4821-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.4821\n",
            "Epoch 109/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.4670\n",
            "Epoch 109: loss improved from 1.48208 to 1.46706, saving model to /content/drive/MyDrive/156 materials/weights-improvement-109-1.4671-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.4671\n",
            "Epoch 110/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.4550\n",
            "Epoch 110: loss improved from 1.46706 to 1.45491, saving model to /content/drive/MyDrive/156 materials/weights-improvement-110-1.4549-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.4549\n",
            "Epoch 111/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.4284\n",
            "Epoch 111: loss improved from 1.45491 to 1.42847, saving model to /content/drive/MyDrive/156 materials/weights-improvement-111-1.4285-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.4285\n",
            "Epoch 112/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.4269\n",
            "Epoch 112: loss improved from 1.42847 to 1.42691, saving model to /content/drive/MyDrive/156 materials/weights-improvement-112-1.4269-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.4269\n",
            "Epoch 113/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.4032\n",
            "Epoch 113: loss improved from 1.42691 to 1.40332, saving model to /content/drive/MyDrive/156 materials/weights-improvement-113-1.4033-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.4033\n",
            "Epoch 114/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.3953\n",
            "Epoch 114: loss improved from 1.40332 to 1.39526, saving model to /content/drive/MyDrive/156 materials/weights-improvement-114-1.3953-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.3953\n",
            "Epoch 115/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 1.3837\n",
            "Epoch 115: loss improved from 1.39526 to 1.38370, saving model to /content/drive/MyDrive/156 materials/weights-improvement-115-1.3837-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.3837\n",
            "Epoch 116/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.3723\n",
            "Epoch 116: loss improved from 1.38370 to 1.37249, saving model to /content/drive/MyDrive/156 materials/weights-improvement-116-1.3725-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.3725\n",
            "Epoch 117/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.3691\n",
            "Epoch 117: loss improved from 1.37249 to 1.36908, saving model to /content/drive/MyDrive/156 materials/weights-improvement-117-1.3691-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.3691\n",
            "Epoch 118/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.3589\n",
            "Epoch 118: loss improved from 1.36908 to 1.35941, saving model to /content/drive/MyDrive/156 materials/weights-improvement-118-1.3594-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.3594\n",
            "Epoch 119/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.3389\n",
            "Epoch 119: loss improved from 1.35941 to 1.33887, saving model to /content/drive/MyDrive/156 materials/weights-improvement-119-1.3389-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.3389\n",
            "Epoch 120/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.3307\n",
            "Epoch 120: loss improved from 1.33887 to 1.33053, saving model to /content/drive/MyDrive/156 materials/weights-improvement-120-1.3305-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.3305\n",
            "Epoch 121/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.3222\n",
            "Epoch 121: loss improved from 1.33053 to 1.32218, saving model to /content/drive/MyDrive/156 materials/weights-improvement-121-1.3222-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.3222\n",
            "Epoch 122/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.3030\n",
            "Epoch 122: loss improved from 1.32218 to 1.30301, saving model to /content/drive/MyDrive/156 materials/weights-improvement-122-1.3030-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.3030\n",
            "Epoch 123/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.2972\n",
            "Epoch 123: loss improved from 1.30301 to 1.29731, saving model to /content/drive/MyDrive/156 materials/weights-improvement-123-1.2973-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.2973\n",
            "Epoch 124/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.2810\n",
            "Epoch 124: loss improved from 1.29731 to 1.28116, saving model to /content/drive/MyDrive/156 materials/weights-improvement-124-1.2812-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.2812\n",
            "Epoch 125/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 1.2774\n",
            "Epoch 125: loss improved from 1.28116 to 1.27742, saving model to /content/drive/MyDrive/156 materials/weights-improvement-125-1.2774-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.2774\n",
            "Epoch 126/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 1.2594\n",
            "Epoch 126: loss improved from 1.27742 to 1.25958, saving model to /content/drive/MyDrive/156 materials/weights-improvement-126-1.2596-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.2596\n",
            "Epoch 127/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.2506\n",
            "Epoch 127: loss improved from 1.25958 to 1.25089, saving model to /content/drive/MyDrive/156 materials/weights-improvement-127-1.2509-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.2509\n",
            "Epoch 128/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.2437\n",
            "Epoch 128: loss improved from 1.25089 to 1.24366, saving model to /content/drive/MyDrive/156 materials/weights-improvement-128-1.2437-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.2437\n",
            "Epoch 129/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.2269\n",
            "Epoch 129: loss improved from 1.24366 to 1.22699, saving model to /content/drive/MyDrive/156 materials/weights-improvement-129-1.2270-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.2270\n",
            "Epoch 130/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.2221\n",
            "Epoch 130: loss improved from 1.22699 to 1.22202, saving model to /content/drive/MyDrive/156 materials/weights-improvement-130-1.2220-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.2220\n",
            "Epoch 131/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 1.2104\n",
            "Epoch 131: loss improved from 1.22202 to 1.21037, saving model to /content/drive/MyDrive/156 materials/weights-improvement-131-1.2104-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.2104\n",
            "Epoch 132/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.1944\n",
            "Epoch 132: loss improved from 1.21037 to 1.19466, saving model to /content/drive/MyDrive/156 materials/weights-improvement-132-1.1947-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.1947\n",
            "Epoch 133/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 1.1973\n",
            "Epoch 133: loss did not improve from 1.19466\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.1972\n",
            "Epoch 134/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.1948\n",
            "Epoch 134: loss did not improve from 1.19466\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.1947\n",
            "Epoch 135/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.1793\n",
            "Epoch 135: loss improved from 1.19466 to 1.17952, saving model to /content/drive/MyDrive/156 materials/weights-improvement-135-1.1795-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.1795\n",
            "Epoch 136/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.1660\n",
            "Epoch 136: loss improved from 1.17952 to 1.16625, saving model to /content/drive/MyDrive/156 materials/weights-improvement-136-1.1663-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.1663\n",
            "Epoch 137/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.1517\n",
            "Epoch 137: loss improved from 1.16625 to 1.15177, saving model to /content/drive/MyDrive/156 materials/weights-improvement-137-1.1518-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.1518\n",
            "Epoch 138/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 1.1403\n",
            "Epoch 138: loss improved from 1.15177 to 1.14047, saving model to /content/drive/MyDrive/156 materials/weights-improvement-138-1.1405-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.1405\n",
            "Epoch 139/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 1.1409\n",
            "Epoch 139: loss did not improve from 1.14047\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.1409\n",
            "Epoch 140/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 1.1429\n",
            "Epoch 140: loss did not improve from 1.14047\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.1429\n",
            "Epoch 141/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.1222\n",
            "Epoch 141: loss improved from 1.14047 to 1.12246, saving model to /content/drive/MyDrive/156 materials/weights-improvement-141-1.1225-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.1225\n",
            "Epoch 142/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.1195\n",
            "Epoch 142: loss improved from 1.12246 to 1.11987, saving model to /content/drive/MyDrive/156 materials/weights-improvement-142-1.1199-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.1199\n",
            "Epoch 143/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.1162\n",
            "Epoch 143: loss improved from 1.11987 to 1.11619, saving model to /content/drive/MyDrive/156 materials/weights-improvement-143-1.1162-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.1162\n",
            "Epoch 144/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.0988\n",
            "Epoch 144: loss improved from 1.11619 to 1.09931, saving model to /content/drive/MyDrive/156 materials/weights-improvement-144-1.0993-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0993\n",
            "Epoch 145/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.0975\n",
            "Epoch 145: loss improved from 1.09931 to 1.09783, saving model to /content/drive/MyDrive/156 materials/weights-improvement-145-1.0978-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0978\n",
            "Epoch 146/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.0895\n",
            "Epoch 146: loss improved from 1.09783 to 1.08940, saving model to /content/drive/MyDrive/156 materials/weights-improvement-146-1.0894-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0894\n",
            "Epoch 147/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 1.0696\n",
            "Epoch 147: loss improved from 1.08940 to 1.06966, saving model to /content/drive/MyDrive/156 materials/weights-improvement-147-1.0697-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0697\n",
            "Epoch 148/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 1.0666\n",
            "Epoch 148: loss improved from 1.06966 to 1.06663, saving model to /content/drive/MyDrive/156 materials/weights-improvement-148-1.0666-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.0666\n",
            "Epoch 149/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.0596\n",
            "Epoch 149: loss improved from 1.06663 to 1.05958, saving model to /content/drive/MyDrive/156 materials/weights-improvement-149-1.0596-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0596\n",
            "Epoch 150/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.0513\n",
            "Epoch 150: loss improved from 1.05958 to 1.05124, saving model to /content/drive/MyDrive/156 materials/weights-improvement-150-1.0512-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0512\n",
            "Epoch 151/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 1.0469\n",
            "Epoch 151: loss improved from 1.05124 to 1.04687, saving model to /content/drive/MyDrive/156 materials/weights-improvement-151-1.0469-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0469\n",
            "Epoch 152/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 1.0415\n",
            "Epoch 152: loss improved from 1.04687 to 1.04153, saving model to /content/drive/MyDrive/156 materials/weights-improvement-152-1.0415-bigger.hdf5\n",
            "3883/3883 [==============================] - 34s 9ms/step - loss: 1.0415\n",
            "Epoch 153/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.0299\n",
            "Epoch 153: loss improved from 1.04153 to 1.02977, saving model to /content/drive/MyDrive/156 materials/weights-improvement-153-1.0298-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0298\n",
            "Epoch 154/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.0319\n",
            "Epoch 154: loss did not improve from 1.02977\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0322\n",
            "Epoch 155/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 1.0193\n",
            "Epoch 155: loss improved from 1.02977 to 1.01949, saving model to /content/drive/MyDrive/156 materials/weights-improvement-155-1.0195-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0195\n",
            "Epoch 156/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 1.0079\n",
            "Epoch 156: loss improved from 1.01949 to 1.00801, saving model to /content/drive/MyDrive/156 materials/weights-improvement-156-1.0080-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0080\n",
            "Epoch 157/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 1.0022\n",
            "Epoch 157: loss improved from 1.00801 to 1.00230, saving model to /content/drive/MyDrive/156 materials/weights-improvement-157-1.0023-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 1.0023\n",
            "Epoch 158/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.9928\n",
            "Epoch 158: loss improved from 1.00230 to 0.99303, saving model to /content/drive/MyDrive/156 materials/weights-improvement-158-0.9930-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9930\n",
            "Epoch 159/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.9907\n",
            "Epoch 159: loss improved from 0.99303 to 0.99078, saving model to /content/drive/MyDrive/156 materials/weights-improvement-159-0.9908-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9908\n",
            "Epoch 160/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.9890\n",
            "Epoch 160: loss improved from 0.99078 to 0.98903, saving model to /content/drive/MyDrive/156 materials/weights-improvement-160-0.9890-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.9890\n",
            "Epoch 161/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.9826\n",
            "Epoch 161: loss improved from 0.98903 to 0.98259, saving model to /content/drive/MyDrive/156 materials/weights-improvement-161-0.9826-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9826\n",
            "Epoch 162/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.9710\n",
            "Epoch 162: loss improved from 0.98259 to 0.97096, saving model to /content/drive/MyDrive/156 materials/weights-improvement-162-0.9710-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9710\n",
            "Epoch 163/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.9586\n",
            "Epoch 163: loss improved from 0.97096 to 0.95859, saving model to /content/drive/MyDrive/156 materials/weights-improvement-163-0.9586-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9586\n",
            "Epoch 164/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.9576\n",
            "Epoch 164: loss improved from 0.95859 to 0.95770, saving model to /content/drive/MyDrive/156 materials/weights-improvement-164-0.9577-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9577\n",
            "Epoch 165/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.9416\n",
            "Epoch 165: loss improved from 0.95770 to 0.94173, saving model to /content/drive/MyDrive/156 materials/weights-improvement-165-0.9417-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9417\n",
            "Epoch 166/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.9487\n",
            "Epoch 166: loss did not improve from 0.94173\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.9492\n",
            "Epoch 167/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.9482\n",
            "Epoch 167: loss did not improve from 0.94173\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9483\n",
            "Epoch 168/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.9308\n",
            "Epoch 168: loss improved from 0.94173 to 0.93076, saving model to /content/drive/MyDrive/156 materials/weights-improvement-168-0.9308-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.9308\n",
            "Epoch 169/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.9257\n",
            "Epoch 169: loss improved from 0.93076 to 0.92571, saving model to /content/drive/MyDrive/156 materials/weights-improvement-169-0.9257-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9257\n",
            "Epoch 170/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.9288\n",
            "Epoch 170: loss did not improve from 0.92571\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9290\n",
            "Epoch 171/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.9214\n",
            "Epoch 171: loss improved from 0.92571 to 0.92138, saving model to /content/drive/MyDrive/156 materials/weights-improvement-171-0.9214-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9214\n",
            "Epoch 172/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.9072\n",
            "Epoch 172: loss improved from 0.92138 to 0.90734, saving model to /content/drive/MyDrive/156 materials/weights-improvement-172-0.9073-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9073\n",
            "Epoch 173/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.9184\n",
            "Epoch 173: loss did not improve from 0.90734\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.9188\n",
            "Epoch 174/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.9027\n",
            "Epoch 174: loss improved from 0.90734 to 0.90273, saving model to /content/drive/MyDrive/156 materials/weights-improvement-174-0.9027-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.9027\n",
            "Epoch 175/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 0.8936\n",
            "Epoch 175: loss improved from 0.90273 to 0.89363, saving model to /content/drive/MyDrive/156 materials/weights-improvement-175-0.8936-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8936\n",
            "Epoch 176/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 0.8982\n",
            "Epoch 176: loss did not improve from 0.89363\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.8983\n",
            "Epoch 177/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.8878\n",
            "Epoch 177: loss improved from 0.89363 to 0.88796, saving model to /content/drive/MyDrive/156 materials/weights-improvement-177-0.8880-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.8880\n",
            "Epoch 178/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.8798\n",
            "Epoch 178: loss improved from 0.88796 to 0.87973, saving model to /content/drive/MyDrive/156 materials/weights-improvement-178-0.8797-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.8797\n",
            "Epoch 179/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.8850\n",
            "Epoch 179: loss did not improve from 0.87973\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.8850\n",
            "Epoch 180/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.8645\n",
            "Epoch 180: loss improved from 0.87973 to 0.86447, saving model to /content/drive/MyDrive/156 materials/weights-improvement-180-0.8645-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8645\n",
            "Epoch 181/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.8699\n",
            "Epoch 181: loss did not improve from 0.86447\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.8698\n",
            "Epoch 182/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.8555\n",
            "Epoch 182: loss improved from 0.86447 to 0.85574, saving model to /content/drive/MyDrive/156 materials/weights-improvement-182-0.8557-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8557\n",
            "Epoch 183/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.8498\n",
            "Epoch 183: loss improved from 0.85574 to 0.85005, saving model to /content/drive/MyDrive/156 materials/weights-improvement-183-0.8501-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8501\n",
            "Epoch 184/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.8543\n",
            "Epoch 184: loss did not improve from 0.85005\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8543\n",
            "Epoch 185/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.8510\n",
            "Epoch 185: loss did not improve from 0.85005\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8512\n",
            "Epoch 186/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.8502\n",
            "Epoch 186: loss did not improve from 0.85005\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.8504\n",
            "Epoch 187/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.8429\n",
            "Epoch 187: loss improved from 0.85005 to 0.84299, saving model to /content/drive/MyDrive/156 materials/weights-improvement-187-0.8430-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8430\n",
            "Epoch 188/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.8370\n",
            "Epoch 188: loss improved from 0.84299 to 0.83698, saving model to /content/drive/MyDrive/156 materials/weights-improvement-188-0.8370-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8370\n",
            "Epoch 189/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.8265\n",
            "Epoch 189: loss improved from 0.83698 to 0.82662, saving model to /content/drive/MyDrive/156 materials/weights-improvement-189-0.8266-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8266\n",
            "Epoch 190/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.8214\n",
            "Epoch 190: loss improved from 0.82662 to 0.82136, saving model to /content/drive/MyDrive/156 materials/weights-improvement-190-0.8214-bigger.hdf5\n",
            "3883/3883 [==============================] - 37s 9ms/step - loss: 0.8214\n",
            "Epoch 191/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.8217\n",
            "Epoch 191: loss did not improve from 0.82136\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8217\n",
            "Epoch 192/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.8170\n",
            "Epoch 192: loss improved from 0.82136 to 0.81715, saving model to /content/drive/MyDrive/156 materials/weights-improvement-192-0.8172-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8172\n",
            "Epoch 193/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.8117\n",
            "Epoch 193: loss improved from 0.81715 to 0.81203, saving model to /content/drive/MyDrive/156 materials/weights-improvement-193-0.8120-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8120\n",
            "Epoch 194/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.8054\n",
            "Epoch 194: loss improved from 0.81203 to 0.80544, saving model to /content/drive/MyDrive/156 materials/weights-improvement-194-0.8054-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8054\n",
            "Epoch 195/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.8031\n",
            "Epoch 195: loss improved from 0.80544 to 0.80308, saving model to /content/drive/MyDrive/156 materials/weights-improvement-195-0.8031-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8031\n",
            "Epoch 196/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.7920\n",
            "Epoch 196: loss improved from 0.80308 to 0.79200, saving model to /content/drive/MyDrive/156 materials/weights-improvement-196-0.7920-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.7920\n",
            "Epoch 197/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.8068\n",
            "Epoch 197: loss did not improve from 0.79200\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.8067\n",
            "Epoch 198/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.7908\n",
            "Epoch 198: loss improved from 0.79200 to 0.79081, saving model to /content/drive/MyDrive/156 materials/weights-improvement-198-0.7908-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7908\n",
            "Epoch 199/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.7880\n",
            "Epoch 199: loss improved from 0.79081 to 0.78797, saving model to /content/drive/MyDrive/156 materials/weights-improvement-199-0.7880-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7880\n",
            "Epoch 200/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.7782\n",
            "Epoch 200: loss improved from 0.78797 to 0.77818, saving model to /content/drive/MyDrive/156 materials/weights-improvement-200-0.7782-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7782\n",
            "Epoch 201/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.7818\n",
            "Epoch 201: loss did not improve from 0.77818\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7818\n",
            "Epoch 202/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.7829\n",
            "Epoch 202: loss did not improve from 0.77818\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7829\n",
            "Epoch 203/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.7700\n",
            "Epoch 203: loss improved from 0.77818 to 0.77003, saving model to /content/drive/MyDrive/156 materials/weights-improvement-203-0.7700-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7700\n",
            "Epoch 204/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7737\n",
            "Epoch 204: loss did not improve from 0.77003\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7736\n",
            "Epoch 205/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.7619\n",
            "Epoch 205: loss improved from 0.77003 to 0.76188, saving model to /content/drive/MyDrive/156 materials/weights-improvement-205-0.7619-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7619\n",
            "Epoch 206/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.7656\n",
            "Epoch 206: loss did not improve from 0.76188\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7658\n",
            "Epoch 207/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7603\n",
            "Epoch 207: loss improved from 0.76188 to 0.76066, saving model to /content/drive/MyDrive/156 materials/weights-improvement-207-0.7607-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7607\n",
            "Epoch 208/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.7531\n",
            "Epoch 208: loss improved from 0.76066 to 0.75300, saving model to /content/drive/MyDrive/156 materials/weights-improvement-208-0.7530-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7530\n",
            "Epoch 209/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7562\n",
            "Epoch 209: loss did not improve from 0.75300\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7563\n",
            "Epoch 210/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.7511\n",
            "Epoch 210: loss improved from 0.75300 to 0.75106, saving model to /content/drive/MyDrive/156 materials/weights-improvement-210-0.7511-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7511\n",
            "Epoch 211/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.7507\n",
            "Epoch 211: loss improved from 0.75106 to 0.75079, saving model to /content/drive/MyDrive/156 materials/weights-improvement-211-0.7508-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7508\n",
            "Epoch 212/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.7439\n",
            "Epoch 212: loss improved from 0.75079 to 0.74403, saving model to /content/drive/MyDrive/156 materials/weights-improvement-212-0.7440-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7440\n",
            "Epoch 213/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7340\n",
            "Epoch 213: loss improved from 0.74403 to 0.73399, saving model to /content/drive/MyDrive/156 materials/weights-improvement-213-0.7340-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7340\n",
            "Epoch 214/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.7454\n",
            "Epoch 214: loss did not improve from 0.73399\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7454\n",
            "Epoch 215/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.7165\n",
            "Epoch 215: loss improved from 0.73399 to 0.71662, saving model to /content/drive/MyDrive/156 materials/weights-improvement-215-0.7166-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7166\n",
            "Epoch 216/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7226\n",
            "Epoch 216: loss did not improve from 0.71662\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7227\n",
            "Epoch 217/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.7240\n",
            "Epoch 217: loss did not improve from 0.71662\n",
            "3883/3883 [==============================] - 37s 10ms/step - loss: 0.7240\n",
            "Epoch 218/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7158\n",
            "Epoch 218: loss improved from 0.71662 to 0.71581, saving model to /content/drive/MyDrive/156 materials/weights-improvement-218-0.7158-bigger.hdf5\n",
            "3883/3883 [==============================] - 38s 10ms/step - loss: 0.7158\n",
            "Epoch 219/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.7221\n",
            "Epoch 219: loss did not improve from 0.71581\n",
            "3883/3883 [==============================] - 37s 9ms/step - loss: 0.7221\n",
            "Epoch 220/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.7114\n",
            "Epoch 220: loss improved from 0.71581 to 0.71140, saving model to /content/drive/MyDrive/156 materials/weights-improvement-220-0.7114-bigger.hdf5\n",
            "3883/3883 [==============================] - 37s 10ms/step - loss: 0.7114\n",
            "Epoch 221/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.7232\n",
            "Epoch 221: loss did not improve from 0.71140\n",
            "3883/3883 [==============================] - 37s 10ms/step - loss: 0.7232\n",
            "Epoch 222/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7118\n",
            "Epoch 222: loss did not improve from 0.71140\n",
            "3883/3883 [==============================] - 37s 10ms/step - loss: 0.7120\n",
            "Epoch 223/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7103\n",
            "Epoch 223: loss improved from 0.71140 to 0.71033, saving model to /content/drive/MyDrive/156 materials/weights-improvement-223-0.7103-bigger.hdf5\n",
            "3883/3883 [==============================] - 37s 10ms/step - loss: 0.7103\n",
            "Epoch 224/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7010\n",
            "Epoch 224: loss improved from 0.71033 to 0.70117, saving model to /content/drive/MyDrive/156 materials/weights-improvement-224-0.7012-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7012\n",
            "Epoch 225/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.7006\n",
            "Epoch 225: loss improved from 0.70117 to 0.70081, saving model to /content/drive/MyDrive/156 materials/weights-improvement-225-0.7008-bigger.hdf5\n",
            "3883/3883 [==============================] - 37s 9ms/step - loss: 0.7008\n",
            "Epoch 226/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.7027\n",
            "Epoch 226: loss did not improve from 0.70081\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.7027\n",
            "Epoch 227/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6937\n",
            "Epoch 227: loss improved from 0.70081 to 0.69372, saving model to /content/drive/MyDrive/156 materials/weights-improvement-227-0.6937-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6937\n",
            "Epoch 228/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6994\n",
            "Epoch 228: loss did not improve from 0.69372\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6994\n",
            "Epoch 229/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6966\n",
            "Epoch 229: loss did not improve from 0.69372\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6967\n",
            "Epoch 230/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6919\n",
            "Epoch 230: loss improved from 0.69372 to 0.69193, saving model to /content/drive/MyDrive/156 materials/weights-improvement-230-0.6919-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6919\n",
            "Epoch 231/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6822\n",
            "Epoch 231: loss improved from 0.69193 to 0.68238, saving model to /content/drive/MyDrive/156 materials/weights-improvement-231-0.6824-bigger.hdf5\n",
            "3883/3883 [==============================] - 37s 9ms/step - loss: 0.6824\n",
            "Epoch 232/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6903\n",
            "Epoch 232: loss did not improve from 0.68238\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6904\n",
            "Epoch 233/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.6814\n",
            "Epoch 233: loss improved from 0.68238 to 0.68134, saving model to /content/drive/MyDrive/156 materials/weights-improvement-233-0.6813-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6813\n",
            "Epoch 234/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.6778\n",
            "Epoch 234: loss improved from 0.68134 to 0.67782, saving model to /content/drive/MyDrive/156 materials/weights-improvement-234-0.6778-bigger.hdf5\n",
            "3883/3883 [==============================] - 37s 9ms/step - loss: 0.6778\n",
            "Epoch 235/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.6750\n",
            "Epoch 235: loss improved from 0.67782 to 0.67497, saving model to /content/drive/MyDrive/156 materials/weights-improvement-235-0.6750-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6750\n",
            "Epoch 236/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.6770\n",
            "Epoch 236: loss did not improve from 0.67497\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6770\n",
            "Epoch 237/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6810\n",
            "Epoch 237: loss did not improve from 0.67497\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6809\n",
            "Epoch 238/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6648\n",
            "Epoch 238: loss improved from 0.67497 to 0.66482, saving model to /content/drive/MyDrive/156 materials/weights-improvement-238-0.6648-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6648\n",
            "Epoch 239/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6602\n",
            "Epoch 239: loss improved from 0.66482 to 0.66059, saving model to /content/drive/MyDrive/156 materials/weights-improvement-239-0.6606-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6606\n",
            "Epoch 240/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.6672\n",
            "Epoch 240: loss did not improve from 0.66059\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6674\n",
            "Epoch 241/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.6670\n",
            "Epoch 241: loss did not improve from 0.66059\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6670\n",
            "Epoch 242/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.6700\n",
            "Epoch 242: loss did not improve from 0.66059\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6700\n",
            "Epoch 243/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.6564\n",
            "Epoch 243: loss improved from 0.66059 to 0.65655, saving model to /content/drive/MyDrive/156 materials/weights-improvement-243-0.6565-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6565\n",
            "Epoch 244/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6557\n",
            "Epoch 244: loss improved from 0.65655 to 0.65567, saving model to /content/drive/MyDrive/156 materials/weights-improvement-244-0.6557-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6557\n",
            "Epoch 245/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.6518\n",
            "Epoch 245: loss improved from 0.65567 to 0.65183, saving model to /content/drive/MyDrive/156 materials/weights-improvement-245-0.6518-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6518\n",
            "Epoch 246/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.6488\n",
            "Epoch 246: loss improved from 0.65183 to 0.64876, saving model to /content/drive/MyDrive/156 materials/weights-improvement-246-0.6488-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6488\n",
            "Epoch 247/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6530\n",
            "Epoch 247: loss did not improve from 0.64876\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6529\n",
            "Epoch 248/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.6445\n",
            "Epoch 248: loss improved from 0.64876 to 0.64450, saving model to /content/drive/MyDrive/156 materials/weights-improvement-248-0.6445-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6445\n",
            "Epoch 249/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.6489\n",
            "Epoch 249: loss did not improve from 0.64450\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6490\n",
            "Epoch 250/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.6439\n",
            "Epoch 250: loss improved from 0.64450 to 0.64387, saving model to /content/drive/MyDrive/156 materials/weights-improvement-250-0.6439-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6439\n",
            "Epoch 251/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.6417\n",
            "Epoch 251: loss improved from 0.64387 to 0.64174, saving model to /content/drive/MyDrive/156 materials/weights-improvement-251-0.6417-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6417\n",
            "Epoch 252/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6334\n",
            "Epoch 252: loss improved from 0.64174 to 0.63333, saving model to /content/drive/MyDrive/156 materials/weights-improvement-252-0.6333-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6333\n",
            "Epoch 253/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.6305\n",
            "Epoch 253: loss improved from 0.63333 to 0.63073, saving model to /content/drive/MyDrive/156 materials/weights-improvement-253-0.6307-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6307\n",
            "Epoch 254/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6309\n",
            "Epoch 254: loss did not improve from 0.63073\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6309\n",
            "Epoch 255/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6335\n",
            "Epoch 255: loss did not improve from 0.63073\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6337\n",
            "Epoch 256/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6291\n",
            "Epoch 256: loss improved from 0.63073 to 0.62906, saving model to /content/drive/MyDrive/156 materials/weights-improvement-256-0.6291-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6291\n",
            "Epoch 257/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6295\n",
            "Epoch 257: loss did not improve from 0.62906\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6294\n",
            "Epoch 258/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.6237\n",
            "Epoch 258: loss improved from 0.62906 to 0.62363, saving model to /content/drive/MyDrive/156 materials/weights-improvement-258-0.6236-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6236\n",
            "Epoch 259/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6227\n",
            "Epoch 259: loss improved from 0.62363 to 0.62280, saving model to /content/drive/MyDrive/156 materials/weights-improvement-259-0.6228-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6228\n",
            "Epoch 260/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.6167\n",
            "Epoch 260: loss improved from 0.62280 to 0.61673, saving model to /content/drive/MyDrive/156 materials/weights-improvement-260-0.6167-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6167\n",
            "Epoch 261/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.6293\n",
            "Epoch 261: loss did not improve from 0.61673\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6295\n",
            "Epoch 262/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6249\n",
            "Epoch 262: loss did not improve from 0.61673\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6248\n",
            "Epoch 263/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6142\n",
            "Epoch 263: loss improved from 0.61673 to 0.61419, saving model to /content/drive/MyDrive/156 materials/weights-improvement-263-0.6142-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6142\n",
            "Epoch 264/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.6148\n",
            "Epoch 264: loss did not improve from 0.61419\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6148\n",
            "Epoch 265/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.6086\n",
            "Epoch 265: loss improved from 0.61419 to 0.60864, saving model to /content/drive/MyDrive/156 materials/weights-improvement-265-0.6086-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6086\n",
            "Epoch 266/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.6078\n",
            "Epoch 266: loss improved from 0.60864 to 0.60799, saving model to /content/drive/MyDrive/156 materials/weights-improvement-266-0.6080-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6080\n",
            "Epoch 267/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 0.6131\n",
            "Epoch 267: loss did not improve from 0.60799\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6130\n",
            "Epoch 268/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6059\n",
            "Epoch 268: loss improved from 0.60799 to 0.60583, saving model to /content/drive/MyDrive/156 materials/weights-improvement-268-0.6058-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6058\n",
            "Epoch 269/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.6069\n",
            "Epoch 269: loss did not improve from 0.60583\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6069\n",
            "Epoch 270/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.6130\n",
            "Epoch 270: loss did not improve from 0.60583\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6130\n",
            "Epoch 271/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.6001\n",
            "Epoch 271: loss improved from 0.60583 to 0.60011, saving model to /content/drive/MyDrive/156 materials/weights-improvement-271-0.6001-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6001\n",
            "Epoch 272/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.5967\n",
            "Epoch 272: loss improved from 0.60011 to 0.59673, saving model to /content/drive/MyDrive/156 materials/weights-improvement-272-0.5967-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5967\n",
            "Epoch 273/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.5951\n",
            "Epoch 273: loss improved from 0.59673 to 0.59513, saving model to /content/drive/MyDrive/156 materials/weights-improvement-273-0.5951-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5951\n",
            "Epoch 274/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.5959\n",
            "Epoch 274: loss did not improve from 0.59513\n",
            "3883/3883 [==============================] - 37s 9ms/step - loss: 0.5959\n",
            "Epoch 275/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.5953\n",
            "Epoch 275: loss did not improve from 0.59513\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5954\n",
            "Epoch 276/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.6015\n",
            "Epoch 276: loss did not improve from 0.59513\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.6015\n",
            "Epoch 277/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.6003\n",
            "Epoch 277: loss did not improve from 0.59513\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.6003\n",
            "Epoch 278/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.5920\n",
            "Epoch 278: loss improved from 0.59513 to 0.59208, saving model to /content/drive/MyDrive/156 materials/weights-improvement-278-0.5921-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5921\n",
            "Epoch 279/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.5933\n",
            "Epoch 279: loss did not improve from 0.59208\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.5933\n",
            "Epoch 280/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.5875\n",
            "Epoch 280: loss improved from 0.59208 to 0.58762, saving model to /content/drive/MyDrive/156 materials/weights-improvement-280-0.5876-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5876\n",
            "Epoch 281/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.5907\n",
            "Epoch 281: loss did not improve from 0.58762\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5907\n",
            "Epoch 282/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.5904\n",
            "Epoch 282: loss did not improve from 0.58762\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.5904\n",
            "Epoch 283/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.5919\n",
            "Epoch 283: loss did not improve from 0.58762\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5919\n",
            "Epoch 284/300\n",
            "3877/3883 [============================>.] - ETA: 0s - loss: 0.5780\n",
            "Epoch 284: loss improved from 0.58762 to 0.57782, saving model to /content/drive/MyDrive/156 materials/weights-improvement-284-0.5778-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.5778\n",
            "Epoch 285/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.5855\n",
            "Epoch 285: loss did not improve from 0.57782\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.5855\n",
            "Epoch 286/300\n",
            "3878/3883 [============================>.] - ETA: 0s - loss: 0.5818\n",
            "Epoch 286: loss did not improve from 0.57782\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5817\n",
            "Epoch 287/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.5799\n",
            "Epoch 287: loss did not improve from 0.57782\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5800\n",
            "Epoch 288/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.5803\n",
            "Epoch 288: loss did not improve from 0.57782\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5803\n",
            "Epoch 289/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.5872\n",
            "Epoch 289: loss did not improve from 0.57782\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.5876\n",
            "Epoch 290/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.5814\n",
            "Epoch 290: loss did not improve from 0.57782\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5814\n",
            "Epoch 291/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.5683\n",
            "Epoch 291: loss improved from 0.57782 to 0.56835, saving model to /content/drive/MyDrive/156 materials/weights-improvement-291-0.5684-bigger.hdf5\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.5684\n",
            "Epoch 292/300\n",
            "3879/3883 [============================>.] - ETA: 0s - loss: 0.5763\n",
            "Epoch 292: loss did not improve from 0.56835\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5762\n",
            "Epoch 293/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.5766\n",
            "Epoch 293: loss did not improve from 0.56835\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5766\n",
            "Epoch 294/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.5696\n",
            "Epoch 294: loss did not improve from 0.56835\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5697\n",
            "Epoch 295/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.5592\n",
            "Epoch 295: loss improved from 0.56835 to 0.55926, saving model to /content/drive/MyDrive/156 materials/weights-improvement-295-0.5593-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5593\n",
            "Epoch 296/300\n",
            "3882/3883 [============================>.] - ETA: 0s - loss: 0.5627\n",
            "Epoch 296: loss did not improve from 0.55926\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5627\n",
            "Epoch 297/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.5602\n",
            "Epoch 297: loss did not improve from 0.55926\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5602\n",
            "Epoch 298/300\n",
            "3881/3883 [============================>.] - ETA: 0s - loss: 0.5636\n",
            "Epoch 298: loss did not improve from 0.55926\n",
            "3883/3883 [==============================] - 35s 9ms/step - loss: 0.5635\n",
            "Epoch 299/300\n",
            "3883/3883 [==============================] - ETA: 0s - loss: 0.5569\n",
            "Epoch 299: loss improved from 0.55926 to 0.55694, saving model to /content/drive/MyDrive/156 materials/weights-improvement-299-0.5569-bigger.hdf5\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5569\n",
            "Epoch 300/300\n",
            "3880/3883 [============================>.] - ETA: 0s - loss: 0.5647\n",
            "Epoch 300: loss did not improve from 0.55694\n",
            "3883/3883 [==============================] - 36s 9ms/step - loss: 0.5647\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c96ea9f2ec0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the model\n",
        "words_model.fit(X, y, epochs=300, batch_size=15, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YatCvvMBYYpD"
      },
      "source": [
        "### Model predictions\n",
        "\n",
        "The final set of weights from epoch 300 was highly overfit, so instead I used for prediction a set of weights from epoch 90, which had captured some patterns from my writing but wasn't copying the training data yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huqRe1-uYYKv"
      },
      "outputs": [],
      "source": [
        "# Loads weights\n",
        "filename_words = \"/content/drive/MyDrive/Minerva/Academic/CS156/assignment 2/words_model/weights-improvement-91-1.7459-bigger.hdf5\"\n",
        "words_model.load_weights(filename_words)\n",
        "words_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZftCMO5hZHz",
        "outputId": "4dd8a287-f7db-42e2-b2be-7e27463c55f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "células com conteúdo para isso usaremos outro protocolo de collection view mas antes precisamos de uma breve explicação imagine que tenhamos 10000 itens para exibir na cv se continuássemos implementando\n",
            "\n",
            "Model generation:\n",
            "os código normalmente iríamos criar uma célula de cada mais de 10000 itens mesmo ter nem células de dor e olhos que jeito o noite de deixar e deixar estava uma não três não o não estridente não ruas do bar de o vai que rosto que o tempo futuro \n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "int_to_word = {index: word for word, index in word_index.items()}\n",
        "\n",
        "def generate_text_words(words_model, sequences, tokenizer, seq_length, total_words, int_to_word, num_words=50):\n",
        "    # pick a random seed\n",
        "    start = np.random.randint(0, len(sequences)-1)\n",
        "    pattern = sequences[start-seq_length:start]\n",
        "    print(\"Seed:\")\n",
        "    print(\" \".join([int_to_word[value] for value in pattern]))\n",
        "    print(\"\\nModel generation:\")\n",
        "\n",
        "    # generate words\n",
        "    for i in range(num_words):\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(total_words)\n",
        "        prediction = words_model.predict(x, verbose=0)\n",
        "        index = np.argmax(prediction)\n",
        "        result = int_to_word[index]\n",
        "        sys.stdout.write(result + \" \")\n",
        "        pattern = np.append(pattern, index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "\n",
        "\n",
        "# Calling the function to generate text using the word-based model\n",
        "generate_text_words(words_model, sequences, tokenizer, seq_length, total_words, int_to_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uKAhGn98S-U"
      },
      "source": [
        "> Sampled seed text:\n",
        "\n",
        "> * \"cells with content for this we will use another collection view protocol but first we need a brief explanation imagine we have 10000 items to display in the cv if we continued implementing\"\n",
        "\n",
        "> Generated output:\n",
        "\n",
        "> - \"the codes normally we would create a cell of each more than 10000 items even though there are no pain cells and eyes that way the night of leaving and leaving was one no three no the no strident no streets of the bar of the go what face that the future time\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NCoasQH9bHd"
      },
      "source": [
        "### Analysis of results\n",
        "\n",
        "There are no grammatical mistakes here — which is expected, since the model is trained on words, and not characters. However, the output is a mix of overfit text and hallucinations. The beginning of the output is the exact continuation of the input, which comes from an iOS tutorial I once wrote. However, at some point, it switches to a nearly-random set of words. This random set resembles some of the novels I wrote, but they don't make sense.\n",
        "\n",
        "It is worth mentioning again that this result comes from the weights the model had around epoch 90. Weights from earlier epochs resulted in text with no meaning, and from later epochs, in copies of the training data due to overfit.\n",
        "\n",
        "The model trained on words seems to be unable to find the balance between learning my writing style, learning to generate text that makes sense, and not overfitting the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5h-HyGi_s46"
      },
      "source": [
        "# Final conclusion\n",
        "\n",
        "it seems that LSTMs can only scratch the surface of text generation. These models output content that individually makes sense (such as characters that make up actual words), but they struggle to arrange these successful units in a meaningful way, apparently being unable to create useful sentences without overfitting the training data.\n",
        "\n",
        "In order to bridge this gap, it seems like we need a model that can understand the relevance of each word relative to each other. This likely means a model that contains attention mechanisms, which I will explore in the next assignment."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
